{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "502e1d85-9407-46ae-91a8-4bf6312fdd27",
   "metadata": {},
   "source": [
    "# Downloading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83322fd3-2087-45aa-ae19-b71974141a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pyyaml\n",
    "import sys, os, distutils.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28842ad7-934f-490f-b632-52fdb26165ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m222.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m150.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m149.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m148.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m219.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d299cbf-1631-4ff7-ad62-02eea3249d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-x1937lup\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-x1937lup\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 9604f5995cc628619f0e4fd913453b4d7d61db3f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /opt/conda/lib/python3.12/site-packages (from detectron2==0.6) (11.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (from detectron2==0.6) (3.10.1)\n",
      "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting termcolor>=1.1 (from detectron2==0.6)\n",
      "  Downloading termcolor-3.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting tabulate (from detectron2==0.6)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting cloudpickle (from detectron2==0.6)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /opt/conda/lib/python3.12/site-packages (from detectron2==0.6) (4.67.1)\n",
      "Collecting tensorboard (from detectron2==0.6)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
      "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: black in /opt/conda/lib/python3.12/site-packages (from detectron2==0.6) (25.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from detectron2==0.6) (24.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from black->detectron2==0.6) (8.1.8)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from black->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from black->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.12/site-packages (from black->detectron2==0.6) (4.3.6)\n",
      "Collecting absl-py>=0.4 (from tensorboard->detectron2==0.6)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->detectron2==0.6)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->detectron2==0.6)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->detectron2==0.6)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard->detectron2==0.6) (75.8.2)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.12/site-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->detectron2==0.6)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->detectron2==0.6)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Downloading termcolor-3.0.0-py3-none-any.whl (6.3 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m163.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for detectron2: filename=detectron2-0.6-cp312-cp312-linux_x86_64.whl size=915251 sha256=c948e869c95538e0a4fa80f40b14fbe3f3631a6c8c4e13705c5613ec21228db1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g9xmvplx/wheels/d3/6e/bd/1969578f1456a6be2d6f083da65c669f450b23b8f3d1ac14c1\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61442 sha256=daf67070935652502658d9a679724006604aee140ca0b041ff37bcd540abdcca\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144592 sha256=cfa9441b60697ab0fc091e9fda173c5d52c105f633b6b88552919911b83fbbf3\n",
      "  Stored in directory: /home/ucloud/.cache/pip/wheels/1f/be/48/13754633f1d08d1fbfc60d5e80ae1e5d7329500477685286cd\n",
      "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, yacs, werkzeug, termcolor, tensorboard-data-server, tabulate, protobuf, portalocker, omegaconf, markdown, grpcio, cloudpickle, absl-py, tensorboard, iopath, hydra-core, pycocotools, fvcore, detectron2\n",
      "Successfully installed absl-py-2.2.1 antlr4-python3-runtime-4.9.3 cloudpickle-3.1.1 detectron2-0.6 fvcore-0.1.5.post20221221 grpcio-1.71.0 hydra-core-1.3.2 iopath-0.1.9 markdown-3.7 omegaconf-2.3.0 portalocker-3.1.1 protobuf-6.30.2 pycocotools-2.0.8 tabulate-0.9.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 termcolor-3.0.0 werkzeug-3.1.3 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "# Detectron 2\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6572845d-159b-440c-9104-edde9aae0cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.2.3)\n",
      "Requirement already satisfied: torch==2.6.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafffde4-fc10-4371-b5b9-52a29105ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from opencv-python) (2.2.3)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m132.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dfe5a-6061-46d3-b50c-a1ab946d8f8d",
   "metadata": {},
   "source": [
    "# Import Downloaded packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c84bec-ff55-4baa-8500-7de35b9424f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: nvcc: not found\n",
      "torch:  2.6 ; cuda:  cu124\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d36056-db7a-4b68-b4f3-1c81faec6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andre pakker\n",
    "\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10d79fd-14bd-48e7-b12f-a7259c46bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import convert_to_coco_json\n",
    "import shutil\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440e8bc-61d8-4c78-889f-fd24fdb5f6f5",
   "metadata": {},
   "source": [
    "# Import images from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f5491d-8b06-4103-94f8-b72cd807af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your GitHub token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned with only the 'Overbite Data' folder.\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import subprocess\n",
    "\n",
    "# Get GitHub token\n",
    "token = getpass(\"Enter your GitHub token: \")\n",
    "\n",
    "# Define repository URL\n",
    "repo_url = f\"https://{token}@github.com/casperbak1/Dataprojekt.git\"\n",
    "\n",
    "# Kloner main branch, og kun seneste commit (depth 1)\n",
    "subprocess.run([\"git\", \"clone\", \"--branch\", \"main\", \"--depth\", \"1\", repo_url])\n",
    "\n",
    "repo_path = \"Dataprojekt\"\n",
    "if os.path.exists(repo_path):  # Hvis der findes en GitHub sti \"Dataprojekt\"\n",
    "    subprocess.run([\"git\", \"sparse-checkout\", \"init\", \"--cone\"], cwd=repo_path) # Så klon mappen \"Data/Clean Data/Overbite Data\"\n",
    "    subprocess.run([\"git\", \"sparse-checkout\", \"set\", \"Data/Clean Data/Overbite Data\"], cwd=repo_path)\n",
    "\n",
    "print(\"Repository cloned with only the 'Overbite Data' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff836de-3636-4e9d-aec0-6c6747959689",
   "metadata": {},
   "source": [
    "# Initialise the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24124013-b5ae-4690-a453-e0f08f9b2bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1588 images with keypoints.\n"
     ]
    }
   ],
   "source": [
    "# Load the annotations from the CSV file\n",
    "ANNOTATIONS_FILE = \"Dataprojekt/Data/Clean Data/Overbite Data/Updated_Labels.csv\"\n",
    "annotations_df = pd.read_csv(ANNOTATIONS_FILE)\n",
    "\n",
    "# Sti til dataen\n",
    "DATASET_PATH = \"Dataprojekt/Data/Clean Data/Overbite Data/Annotated Data Pairs\"\n",
    "\n",
    "def my_dataset_function():\n",
    "    dataset_dicts = []\n",
    "\n",
    "    # Group annotations by filename (in case multiple keypoints exist for an image)\n",
    "    grouped_annotations = annotations_df.groupby(\"Filename\")\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(DATASET_PATH)):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            record = {}\n",
    "            file_path = os.path.join(DATASET_PATH, filename)\n",
    "\n",
    "            # Read image dimensions\n",
    "            height, width = cv2.imread(file_path).shape[:2]\n",
    "\n",
    "            # Initialize the dataset record\n",
    "            record[\"file_name\"] = file_path\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "\n",
    "            # Default empty annotation list\n",
    "            record[\"annotations\"] = []\n",
    "\n",
    "            # Check if the file has keypoint annotations\n",
    "            if filename in grouped_annotations.groups:\n",
    "                keypoints_list = []\n",
    "                for _, row in grouped_annotations.get_group(filename).iterrows():\n",
    "                    x, y = row[\"X\"], row[\"Y\"]\n",
    "                    keypoints_list.append(x)  # X-coordinate\n",
    "                    keypoints_list.append(y)  # Y-coordinate\n",
    "                    keypoints_list.append(2)  # Visibility flag (0=not visible, 1=occluded, 2=visible)\n",
    "\n",
    "                # Create the annotation entry\n",
    "                annotation = {\n",
    "                    \"bbox\": [0, 0, width, height],  # Dummy bbox covering entire image\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": 0,  # If you have multiple classes (Vi har 1)\n",
    "                    \"keypoints\": keypoints_list,\n",
    "                    \"num_keypoints\": len(keypoints_list) // 3\n",
    "                }\n",
    "                record[\"annotations\"].append(annotation)\n",
    "\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "# Register the dataset\n",
    "DatasetCatalog.register(\"Overbite_Data\", my_dataset_function)\n",
    "MetadataCatalog.get(\"Overbite_Data\").set(\n",
    "    thing_classes=[\"object\"],  # Modify for actual class names (Mangler godt navn)\n",
    "    keypoint_names=[\"keypoint\"],  # Name of keypoints (Mangler endnu bedre navn)\n",
    "    keypoint_flip_map=[]  # Add keypoint flip pairs if needed (Nope)\n",
    ")\n",
    "\n",
    "# Test if it works\n",
    "dataset_dicts = DatasetCatalog.get(\"Overbite_Data\")\n",
    "print(f\"Loaded {len(dataset_dicts)} images with keypoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ed359-4fec-4252-8c4b-93d521559eb0",
   "metadata": {},
   "source": [
    "# Initialise the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194269b0-ef66-4c4e-9176-26d8f682b2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(name='Overbite_Validation',\n",
       "          thing_classes=['object'],\n",
       "          keypoint_names=['keypoint'],\n",
       "          keypoint_flip_map=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funktion til at hente verifikationsdata\n",
    "def my_validation_function():\n",
    "    dataset_dicts = []\n",
    "    DATASET_PATH = \"Dataprojekt/Data/Clean Data/Overbite Data/Annotated Verification Data Pairs\"\n",
    "\n",
    "    grouped_annotations = annotations_df.groupby(\"Filename\")\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(DATASET_PATH)):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            record = {}\n",
    "            file_path = os.path.join(DATASET_PATH, filename)\n",
    "            height, width = cv2.imread(file_path).shape[:2]\n",
    "\n",
    "            record[\"file_name\"] = file_path\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "            record[\"annotations\"] = []\n",
    "\n",
    "            if filename in grouped_annotations.groups:\n",
    "                keypoints_list = []\n",
    "                for _, row in grouped_annotations.get_group(filename).iterrows():\n",
    "                    x, y = row[\"X\"], row[\"Y\"]\n",
    "                    keypoints_list.append(x)\n",
    "                    keypoints_list.append(y)\n",
    "                    keypoints_list.append(2)  # Visibility flag\n",
    "\n",
    "                annotation = {\n",
    "                    \"bbox\": [0, 0, width, height],  # Dummy bbox\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": 0,\n",
    "                    \"keypoints\": keypoints_list,\n",
    "                    \"num_keypoints\": len(keypoints_list) // 3\n",
    "                }\n",
    "                record[\"annotations\"].append(annotation)\n",
    "\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "# Registrér valideringsdatasæt\n",
    "DatasetCatalog.register(\"Overbite_Validation\", my_validation_function)\n",
    "MetadataCatalog.get(\"Overbite_Validation\").set(\n",
    "    thing_classes=[\"object\"],\n",
    "    keypoint_names=[\"keypoint\"],\n",
    "    keypoint_flip_map=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae3556-8045-44a5-a053-6f02e7d360a1",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d1ab1-fd36-4d97-a438-7abdbd247fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/01 16:00:10 d2.data.datasets.coco]: \u001b[0mUsing previously cached COCO format annotations at './output/Overbite_Model/Overbite_Validation_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "Validation set converted to COCO format: ./output/Overbite_Model/Overbite_Validation_coco_format.json\n",
      "\u001b[32m[04/01 16:00:11 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (keypoint_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (keypoint_head): KRCNNConvDeconvUpsampleHead(\n",
      "      (conv_fcn1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu1): ReLU()\n",
      "      (conv_fcn2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu2): ReLU()\n",
      "      (conv_fcn3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu3): ReLU()\n",
      "      (conv_fcn4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu4): ReLU()\n",
      "      (conv_fcn5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu5): ReLU()\n",
      "      (conv_fcn6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu6): ReLU()\n",
      "      (conv_fcn7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu7): ReLU()\n",
      "      (conv_fcn8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn_relu8): ReLU()\n",
      "      (score_lowres): ConvTranspose2d(512, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[04/01 16:00:24 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1588 images left.\n",
      "\u001b[32m[04/01 16:00:24 d2.data.build]: \u001b[0mRemoved 0 images with fewer than 1 keypoints.\n",
      "\u001b[32m[04/01 16:00:24 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[04/01 16:00:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/01 16:00:24 d2.data.common]: \u001b[0mSerializing 1588 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/01 16:00:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.48 MiB\n",
      "\u001b[32m[04/01 16:00:24 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=16\n",
      "\u001b[32m[04/01 16:00:24 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x/139686956/model_final_5ad38f.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.keypoint_head.score_lowres.weight' to the model due to incompatible shapes: (512, 17, 4, 4) in the checkpoint but (512, 1, 4, 4) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.keypoint_head.score_lowres.bias' to the model due to incompatible shapes: (17,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.keypoint_head.score_lowres.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/01 16:00:24 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n",
      "/opt/conda/lib/python3.12/site-packages/detectron2/structures/keypoints.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/01 16:01:12 d2.utils.events]: \u001b[0m eta: 8:07:54  iter: 19  total_loss: 8.492  loss_cls: 0.166  loss_box_reg: 0.146  loss_keypoint: 7.924  loss_rpn_cls: 0.2137  loss_rpn_loc: 0.04517    time: 2.5854  last_time: 0.3222  data_time: 0.0602  last_data_time: 0.0397   lr: 3.9962e-05  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:18 d2.utils.events]: \u001b[0m eta: 8:07:13  iter: 39  total_loss: 7.049  loss_cls: 0.02733  loss_box_reg: 0.06984  loss_keypoint: 6.892  loss_rpn_cls: 0.01466  loss_rpn_loc: 0.02464    time: 1.3956  last_time: 0.3252  data_time: 0.0410  last_data_time: 0.0439   lr: 7.9922e-05  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:25 d2.utils.events]: \u001b[0m eta: 8:06:13  iter: 59  total_loss: 3.911  loss_cls: 0.01537  loss_box_reg: 0.02569  loss_keypoint: 3.844  loss_rpn_cls: 0.005463  loss_rpn_loc: 0.02222    time: 1.0265  last_time: 0.3250  data_time: 0.0417  last_data_time: 0.0409   lr: 0.00011988  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:31 d2.utils.events]: \u001b[0m eta: 8:05:32  iter: 79  total_loss: 2.196  loss_cls: 0.01415  loss_box_reg: 0.02421  loss_keypoint: 2.13  loss_rpn_cls: 0.006019  loss_rpn_loc: 0.0195    time: 0.8467  last_time: 0.3226  data_time: 0.0433  last_data_time: 0.0398   lr: 0.00015984  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:38 d2.utils.events]: \u001b[0m eta: 8:05:54  iter: 99  total_loss: 1.418  loss_cls: 0.01251  loss_box_reg: 0.01506  loss_keypoint: 1.361  loss_rpn_cls: 0.004372  loss_rpn_loc: 0.01709    time: 0.7421  last_time: 0.3472  data_time: 0.0512  last_data_time: 0.0633   lr: 0.0001998  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:45 d2.utils.events]: \u001b[0m eta: 8:06:10  iter: 119  total_loss: 1.413  loss_cls: 0.01068  loss_box_reg: 0.01053  loss_keypoint: 1.373  loss_rpn_cls: 0.003705  loss_rpn_loc: 0.0143    time: 0.6727  last_time: 0.3229  data_time: 0.0534  last_data_time: 0.0409   lr: 0.00023976  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:51 d2.utils.events]: \u001b[0m eta: 8:06:41  iter: 139  total_loss: 1.139  loss_cls: 0.008008  loss_box_reg: 0.009196  loss_keypoint: 1.109  loss_rpn_cls: 0.003412  loss_rpn_loc: 0.01292    time: 0.6238  last_time: 0.3395  data_time: 0.0583  last_data_time: 0.0597   lr: 0.00027972  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:01:58 d2.utils.events]: \u001b[0m eta: 8:06:38  iter: 159  total_loss: 1.174  loss_cls: 0.007893  loss_box_reg: 0.007571  loss_keypoint: 1.147  loss_rpn_cls: 0.003174  loss_rpn_loc: 0.01161    time: 0.5868  last_time: 0.3414  data_time: 0.0536  last_data_time: 0.0641   lr: 0.00031968  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:05 d2.utils.events]: \u001b[0m eta: 8:06:31  iter: 179  total_loss: 1.065  loss_cls: 0.006852  loss_box_reg: 0.006332  loss_keypoint: 1.041  loss_rpn_cls: 0.00292  loss_rpn_loc: 0.01012    time: 0.5576  last_time: 0.3428  data_time: 0.0494  last_data_time: 0.0619   lr: 0.00035964  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:11 d2.utils.events]: \u001b[0m eta: 8:06:39  iter: 199  total_loss: 1.011  loss_cls: 0.006484  loss_box_reg: 0.005249  loss_keypoint: 0.9861  loss_rpn_cls: 0.002713  loss_rpn_loc: 0.009949    time: 0.5348  last_time: 0.3173  data_time: 0.0538  last_data_time: 0.0406   lr: 0.0003996  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:18 d2.utils.events]: \u001b[0m eta: 8:07:03  iter: 219  total_loss: 0.8787  loss_cls: 0.006321  loss_box_reg: 0.004932  loss_keypoint: 0.859  loss_rpn_cls: 0.002022  loss_rpn_loc: 0.009014    time: 0.5163  last_time: 0.3403  data_time: 0.0557  last_data_time: 0.0641   lr: 0.00043956  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:25 d2.utils.events]: \u001b[0m eta: 8:08:11  iter: 239  total_loss: 1.022  loss_cls: 0.00596  loss_box_reg: 0.004311  loss_keypoint: 1.002  loss_rpn_cls: 0.001895  loss_rpn_loc: 0.008761    time: 0.5007  last_time: 0.3415  data_time: 0.0538  last_data_time: 0.0652   lr: 0.00047952  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:31 d2.utils.events]: \u001b[0m eta: 8:07:51  iter: 259  total_loss: 0.9516  loss_cls: 0.005555  loss_box_reg: 0.003764  loss_keypoint: 0.9331  loss_rpn_cls: 0.00253  loss_rpn_loc: 0.008161    time: 0.4871  last_time: 0.3456  data_time: 0.0492  last_data_time: 0.0680   lr: 0.00051948  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:38 d2.utils.events]: \u001b[0m eta: 8:07:44  iter: 279  total_loss: 0.9389  loss_cls: 0.004998  loss_box_reg: 0.003408  loss_keypoint: 0.9225  loss_rpn_cls: 0.002008  loss_rpn_loc: 0.007876    time: 0.4756  last_time: 0.3216  data_time: 0.0506  last_data_time: 0.0433   lr: 0.00055944  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:44 d2.utils.events]: \u001b[0m eta: 8:08:10  iter: 299  total_loss: 0.848  loss_cls: 0.005136  loss_box_reg: 0.003324  loss_keypoint: 0.8283  loss_rpn_cls: 0.001787  loss_rpn_loc: 0.007669    time: 0.4657  last_time: 0.3182  data_time: 0.0519  last_data_time: 0.0404   lr: 0.0005994  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:51 d2.utils.events]: \u001b[0m eta: 8:07:45  iter: 319  total_loss: 0.8467  loss_cls: 0.005788  loss_box_reg: 0.003197  loss_keypoint: 0.8267  loss_rpn_cls: 0.002058  loss_rpn_loc: 0.007218    time: 0.4569  last_time: 0.3196  data_time: 0.0490  last_data_time: 0.0405   lr: 0.00063936  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:02:57 d2.utils.events]: \u001b[0m eta: 8:07:57  iter: 339  total_loss: 0.9284  loss_cls: 0.004604  loss_box_reg: 0.002894  loss_keypoint: 0.9141  loss_rpn_cls: 0.001575  loss_rpn_loc: 0.007004    time: 0.4492  last_time: 0.3213  data_time: 0.0510  last_data_time: 0.0409   lr: 0.00067932  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:04 d2.utils.events]: \u001b[0m eta: 8:07:59  iter: 359  total_loss: 0.7925  loss_cls: 0.004842  loss_box_reg: 0.002388  loss_keypoint: 0.7766  loss_rpn_cls: 0.001217  loss_rpn_loc: 0.006531    time: 0.4424  last_time: 0.3270  data_time: 0.0506  last_data_time: 0.0478   lr: 0.00071928  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:10 d2.utils.events]: \u001b[0m eta: 8:08:21  iter: 379  total_loss: 0.8734  loss_cls: 0.004548  loss_box_reg: 0.002311  loss_keypoint: 0.859  loss_rpn_cls: 0.001182  loss_rpn_loc: 0.006455    time: 0.4365  last_time: 0.3445  data_time: 0.0542  last_data_time: 0.0679   lr: 0.00075924  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:17 d2.utils.events]: \u001b[0m eta: 8:07:47  iter: 399  total_loss: 1.011  loss_cls: 0.004277  loss_box_reg: 0.002948  loss_keypoint: 0.9964  loss_rpn_cls: 0.00132  loss_rpn_loc: 0.006466    time: 0.4309  last_time: 0.3265  data_time: 0.0487  last_data_time: 0.0446   lr: 0.0007992  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:24 d2.utils.events]: \u001b[0m eta: 8:07:53  iter: 419  total_loss: 0.8096  loss_cls: 0.00396  loss_box_reg: 0.002931  loss_keypoint: 0.7934  loss_rpn_cls: 0.001202  loss_rpn_loc: 0.006586    time: 0.4261  last_time: 0.3205  data_time: 0.0526  last_data_time: 0.0438   lr: 0.00083916  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:30 d2.utils.events]: \u001b[0m eta: 8:08:01  iter: 439  total_loss: 0.781  loss_cls: 0.003946  loss_box_reg: 0.002264  loss_keypoint: 0.7688  loss_rpn_cls: 0.001282  loss_rpn_loc: 0.005745    time: 0.4217  last_time: 0.3197  data_time: 0.0531  last_data_time: 0.0418   lr: 0.00087912  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:37 d2.utils.events]: \u001b[0m eta: 8:07:26  iter: 459  total_loss: 0.8074  loss_cls: 0.004029  loss_box_reg: 0.002031  loss_keypoint: 0.7947  loss_rpn_cls: 0.0009602  loss_rpn_loc: 0.005584    time: 0.4175  last_time: 0.3180  data_time: 0.0476  last_data_time: 0.0412   lr: 0.00091908  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:43 d2.utils.events]: \u001b[0m eta: 8:07:21  iter: 479  total_loss: 0.8007  loss_cls: 0.003358  loss_box_reg: 0.00192  loss_keypoint: 0.7869  loss_rpn_cls: 0.0009181  loss_rpn_loc: 0.005827    time: 0.4138  last_time: 0.3198  data_time: 0.0512  last_data_time: 0.0424   lr: 0.00095904  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:50 d2.utils.events]: \u001b[0m eta: 8:07:02  iter: 499  total_loss: 0.8234  loss_cls: 0.003413  loss_box_reg: 0.001892  loss_keypoint: 0.8118  loss_rpn_cls: 0.0009689  loss_rpn_loc: 0.005741    time: 0.4102  last_time: 0.3366  data_time: 0.0457  last_data_time: 0.0599   lr: 0.000999  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:03:56 d2.utils.events]: \u001b[0m eta: 8:07:01  iter: 519  total_loss: 0.7475  loss_cls: 0.003721  loss_box_reg: 0.002154  loss_keypoint: 0.7357  loss_rpn_cls: 0.001299  loss_rpn_loc: 0.00585    time: 0.4072  last_time: 0.3415  data_time: 0.0530  last_data_time: 0.0615   lr: 0.001039  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:03 d2.utils.events]: \u001b[0m eta: 8:06:58  iter: 539  total_loss: 0.859  loss_cls: 0.003092  loss_box_reg: 0.002251  loss_keypoint: 0.8478  loss_rpn_cls: 0.0009718  loss_rpn_loc: 0.00556    time: 0.4044  last_time: 0.3400  data_time: 0.0523  last_data_time: 0.0605   lr: 0.0010789  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:10 d2.utils.events]: \u001b[0m eta: 8:06:47  iter: 559  total_loss: 0.7918  loss_cls: 0.003093  loss_box_reg: 0.001802  loss_keypoint: 0.7783  loss_rpn_cls: 0.0009411  loss_rpn_loc: 0.005655    time: 0.4018  last_time: 0.3422  data_time: 0.0502  last_data_time: 0.0617   lr: 0.0011189  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:16 d2.utils.events]: \u001b[0m eta: 8:06:20  iter: 579  total_loss: 0.8527  loss_cls: 0.002839  loss_box_reg: 0.001498  loss_keypoint: 0.841  loss_rpn_cls: 0.0008535  loss_rpn_loc: 0.005383    time: 0.3992  last_time: 0.3373  data_time: 0.0489  last_data_time: 0.0596   lr: 0.0011588  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:23 d2.utils.events]: \u001b[0m eta: 8:06:34  iter: 599  total_loss: 0.7683  loss_cls: 0.002802  loss_box_reg: 0.001568  loss_keypoint: 0.7571  loss_rpn_cls: 0.0008715  loss_rpn_loc: 0.005029    time: 0.3970  last_time: 0.3210  data_time: 0.0553  last_data_time: 0.0404   lr: 0.0011988  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:30 d2.utils.events]: \u001b[0m eta: 8:06:30  iter: 619  total_loss: 0.7788  loss_cls: 0.003014  loss_box_reg: 0.001595  loss_keypoint: 0.7687  loss_rpn_cls: 0.0007973  loss_rpn_loc: 0.005103    time: 0.3950  last_time: 0.3412  data_time: 0.0539  last_data_time: 0.0637   lr: 0.0012388  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:36 d2.utils.events]: \u001b[0m eta: 8:06:20  iter: 639  total_loss: 0.6519  loss_cls: 0.002841  loss_box_reg: 0.001529  loss_keypoint: 0.6401  loss_rpn_cls: 0.0008888  loss_rpn_loc: 0.005165    time: 0.3929  last_time: 0.3390  data_time: 0.0481  last_data_time: 0.0621   lr: 0.0012787  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:43 d2.utils.events]: \u001b[0m eta: 8:05:13  iter: 659  total_loss: 0.7341  loss_cls: 0.002913  loss_box_reg: 0.001438  loss_keypoint: 0.7241  loss_rpn_cls: 0.0007496  loss_rpn_loc: 0.004982    time: 0.3908  last_time: 0.3189  data_time: 0.0470  last_data_time: 0.0430   lr: 0.0013187  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:49 d2.utils.events]: \u001b[0m eta: 8:04:30  iter: 679  total_loss: 0.734  loss_cls: 0.002994  loss_box_reg: 0.001667  loss_keypoint: 0.7211  loss_rpn_cls: 0.0008551  loss_rpn_loc: 0.005141    time: 0.3889  last_time: 0.3195  data_time: 0.0467  last_data_time: 0.0425   lr: 0.0013586  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:04:56 d2.utils.events]: \u001b[0m eta: 8:04:24  iter: 699  total_loss: 0.792  loss_cls: 0.00352  loss_box_reg: 0.00179  loss_keypoint: 0.7816  loss_rpn_cls: 0.0007671  loss_rpn_loc: 0.004957    time: 0.3872  last_time: 0.3384  data_time: 0.0516  last_data_time: 0.0603   lr: 0.0013986  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:02 d2.utils.events]: \u001b[0m eta: 8:04:16  iter: 719  total_loss: 0.7776  loss_cls: 0.002808  loss_box_reg: 0.001429  loss_keypoint: 0.7676  loss_rpn_cls: 0.0007433  loss_rpn_loc: 0.004901    time: 0.3856  last_time: 0.3165  data_time: 0.0492  last_data_time: 0.0407   lr: 0.0014386  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:09 d2.utils.events]: \u001b[0m eta: 8:03:44  iter: 739  total_loss: 0.7318  loss_cls: 0.002615  loss_box_reg: 0.001352  loss_keypoint: 0.7217  loss_rpn_cls: 0.0007427  loss_rpn_loc: 0.004627    time: 0.3839  last_time: 0.3344  data_time: 0.0462  last_data_time: 0.0605   lr: 0.0014785  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:15 d2.utils.events]: \u001b[0m eta: 8:03:23  iter: 759  total_loss: 0.6993  loss_cls: 0.003427  loss_box_reg: 0.001343  loss_keypoint: 0.6885  loss_rpn_cls: 0.0009057  loss_rpn_loc: 0.004718    time: 0.3823  last_time: 0.3388  data_time: 0.0459  last_data_time: 0.0611   lr: 0.0015185  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:22 d2.utils.events]: \u001b[0m eta: 8:03:13  iter: 779  total_loss: 0.711  loss_cls: 0.002815  loss_box_reg: 0.001306  loss_keypoint: 0.7022  loss_rpn_cls: 0.0007099  loss_rpn_loc: 0.004487    time: 0.3809  last_time: 0.3217  data_time: 0.0483  last_data_time: 0.0436   lr: 0.0015584  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:29 d2.utils.events]: \u001b[0m eta: 8:03:10  iter: 799  total_loss: 0.679  loss_cls: 0.002554  loss_box_reg: 0.001315  loss_keypoint: 0.6691  loss_rpn_cls: 0.0006891  loss_rpn_loc: 0.004456    time: 0.3796  last_time: 0.3352  data_time: 0.0543  last_data_time: 0.0594   lr: 0.0015984  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:35 d2.utils.events]: \u001b[0m eta: 8:03:00  iter: 819  total_loss: 0.7301  loss_cls: 0.002844  loss_box_reg: 0.001125  loss_keypoint: 0.7213  loss_rpn_cls: 0.0007702  loss_rpn_loc: 0.004684    time: 0.3783  last_time: 0.3213  data_time: 0.0472  last_data_time: 0.0422   lr: 0.0016384  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:42 d2.utils.events]: \u001b[0m eta: 8:02:32  iter: 839  total_loss: 0.6256  loss_cls: 0.00325  loss_box_reg: 0.00107  loss_keypoint: 0.6148  loss_rpn_cls: 0.0006933  loss_rpn_loc: 0.004415    time: 0.3770  last_time: 0.3399  data_time: 0.0462  last_data_time: 0.0609   lr: 0.0016783  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:48 d2.utils.events]: \u001b[0m eta: 8:02:19  iter: 859  total_loss: 0.7031  loss_cls: 0.003015  loss_box_reg: 0.00111  loss_keypoint: 0.6939  loss_rpn_cls: 0.0008131  loss_rpn_loc: 0.004518    time: 0.3758  last_time: 0.3464  data_time: 0.0478  last_data_time: 0.0693   lr: 0.0017183  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:05:55 d2.utils.events]: \u001b[0m eta: 8:02:04  iter: 879  total_loss: 0.7263  loss_cls: 0.002612  loss_box_reg: 0.001034  loss_keypoint: 0.7168  loss_rpn_cls: 0.000746  loss_rpn_loc: 0.004337    time: 0.3747  last_time: 0.3250  data_time: 0.0477  last_data_time: 0.0459   lr: 0.0017582  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:01 d2.utils.events]: \u001b[0m eta: 8:01:53  iter: 899  total_loss: 0.6775  loss_cls: 0.004591  loss_box_reg: 0.001063  loss_keypoint: 0.6639  loss_rpn_cls: 0.0006079  loss_rpn_loc: 0.00429    time: 0.3736  last_time: 0.3185  data_time: 0.0491  last_data_time: 0.0427   lr: 0.0017982  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:08 d2.utils.events]: \u001b[0m eta: 8:01:41  iter: 919  total_loss: 0.6625  loss_cls: 0.003025  loss_box_reg: 0.001246  loss_keypoint: 0.6528  loss_rpn_cls: 0.0005697  loss_rpn_loc: 0.004317    time: 0.3726  last_time: 0.3190  data_time: 0.0491  last_data_time: 0.0404   lr: 0.0018382  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:14 d2.utils.events]: \u001b[0m eta: 8:01:23  iter: 939  total_loss: 0.6481  loss_cls: 0.002589  loss_box_reg: 0.001103  loss_keypoint: 0.6389  loss_rpn_cls: 0.0006383  loss_rpn_loc: 0.004093    time: 0.3716  last_time: 0.3229  data_time: 0.0461  last_data_time: 0.0420   lr: 0.0018781  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:21 d2.utils.events]: \u001b[0m eta: 8:01:21  iter: 959  total_loss: 0.787  loss_cls: 0.002904  loss_box_reg: 0.001126  loss_keypoint: 0.7774  loss_rpn_cls: 0.0007816  loss_rpn_loc: 0.004251    time: 0.3707  last_time: 0.3243  data_time: 0.0477  last_data_time: 0.0405   lr: 0.0019181  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:28 d2.utils.events]: \u001b[0m eta: 8:01:21  iter: 979  total_loss: 0.6615  loss_cls: 0.002588  loss_box_reg: 0.001053  loss_keypoint: 0.6534  loss_rpn_cls: 0.0006569  loss_rpn_loc: 0.003999    time: 0.3699  last_time: 0.3665  data_time: 0.0515  last_data_time: 0.0857   lr: 0.001958  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:37 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   object   | 400          |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[04/01 16:06:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/01 16:06:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[04/01 16:06:37 d2.data.common]: \u001b[0mSerializing 400 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/01 16:06:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/01 16:06:37 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[04/01 16:06:37 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'Overbite_Validation' to COCO format ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/01 16:06:37 d2.data.datasets.coco]: \u001b[0mUsing previously cached COCO format annotations at './output/Overbite_Model/Overbite_Validation_coco_format.json'. You need to clear the cache file if your dataset has been modified.\n",
      "\u001b[32m[04/01 16:06:37 d2.evaluation.evaluator]: \u001b[0mStart inference on 400 batches\n",
      "\u001b[32m[04/01 16:06:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/400. Dataloading: 0.0009 s/iter. Inference: 0.0190 s/iter. Eval: 0.0002 s/iter. Total: 0.0201 s/iter. ETA=0:00:07\n",
      "\u001b[32m[04/01 16:06:43 d2.evaluation.evaluator]: \u001b[0mInference done 267/400. Dataloading: 0.0010 s/iter. Inference: 0.0183 s/iter. Eval: 0.0002 s/iter. Total: 0.0196 s/iter. ETA=0:00:02\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:07.841344 (0.019852 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:07 (0.018312 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/Overbite_Model/coco_instances_results.json\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP    |  AP50   |  AP75   |  APs  |  APm  |   APl   |\n",
      "|:-------:|:-------:|:-------:|:-----:|:-----:|:-------:|\n",
      "| 100.000 | 100.000 | 100.000 |  nan  |  nan  | 100.000 |\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *keypoints*\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 1.000\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for keypoints: \n",
      "|   AP    |  AP50   |  AP75   |  APm  |   APl   |\n",
      "|:-------:|:-------:|:-------:|:-----:|:-------:|\n",
      "| 100.000 | 100.000 | 100.000 |  nan  | 100.000 |\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/01 16:06:46 d2.engine.defaults]: \u001b[0mEvaluation results for Overbite_Validation in csv format:\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: 100.0000,100.0000,100.0000,nan,nan,100.0000\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: Task: keypoints\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APm,APl\n",
      "\u001b[32m[04/01 16:06:46 d2.evaluation.testing]: \u001b[0mcopypaste: 100.0000,100.0000,100.0000,nan,100.0000\n",
      "\u001b[32m[04/01 16:06:46 d2.utils.events]: \u001b[0m eta: 8:01:20  iter: 999  total_loss: 0.6543  loss_cls: 0.003047  loss_box_reg: 0.0009015  loss_keypoint: 0.6464  loss_rpn_cls: 0.0005822  loss_rpn_loc: 0.003947    time: 0.3690  last_time: 0.3197  data_time: 0.0473  last_data_time: 0.0410   lr: 0.001998  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:53 d2.utils.events]: \u001b[0m eta: 8:01:10  iter: 1019  total_loss: 0.5863  loss_cls: 0.003059  loss_box_reg: 0.001093  loss_keypoint: 0.5784  loss_rpn_cls: 0.0006722  loss_rpn_loc: 0.003968    time: 0.3682  last_time: 0.3246  data_time: 0.0497  last_data_time: 0.0467   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:06:59 d2.utils.events]: \u001b[0m eta: 8:00:59  iter: 1039  total_loss: 0.5902  loss_cls: 0.00293  loss_box_reg: 0.001166  loss_keypoint: 0.5803  loss_rpn_cls: 0.0005149  loss_rpn_loc: 0.00393    time: 0.3676  last_time: 0.3478  data_time: 0.0505  last_data_time: 0.0654   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:06 d2.utils.events]: \u001b[0m eta: 8:01:20  iter: 1059  total_loss: 0.6294  loss_cls: 0.002357  loss_box_reg: 0.001003  loss_keypoint: 0.6207  loss_rpn_cls: 0.0005699  loss_rpn_loc: 0.004236    time: 0.3670  last_time: 0.3305  data_time: 0.0565  last_data_time: 0.0532   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:13 d2.utils.events]: \u001b[0m eta: 8:01:44  iter: 1079  total_loss: 0.669  loss_cls: 0.002458  loss_box_reg: 0.0009097  loss_keypoint: 0.661  loss_rpn_cls: 0.0004316  loss_rpn_loc: 0.004224    time: 0.3663  last_time: 0.3282  data_time: 0.0479  last_data_time: 0.0419   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:19 d2.utils.events]: \u001b[0m eta: 8:02:03  iter: 1099  total_loss: 0.6893  loss_cls: 0.003511  loss_box_reg: 0.0009293  loss_keypoint: 0.6799  loss_rpn_cls: 0.0005971  loss_rpn_loc: 0.004232    time: 0.3657  last_time: 0.3270  data_time: 0.0510  last_data_time: 0.0449   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:26 d2.utils.events]: \u001b[0m eta: 8:01:26  iter: 1119  total_loss: 0.6752  loss_cls: 0.002759  loss_box_reg: 0.0009536  loss_keypoint: 0.6665  loss_rpn_cls: 0.0006614  loss_rpn_loc: 0.004035    time: 0.3652  last_time: 0.3406  data_time: 0.0459  last_data_time: 0.0603   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:33 d2.utils.events]: \u001b[0m eta: 8:00:57  iter: 1139  total_loss: 0.6251  loss_cls: 0.003103  loss_box_reg: 0.000805  loss_keypoint: 0.6167  loss_rpn_cls: 0.0005937  loss_rpn_loc: 0.003672    time: 0.3647  last_time: 0.3234  data_time: 0.0509  last_data_time: 0.0403   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:39 d2.utils.events]: \u001b[0m eta: 8:01:07  iter: 1159  total_loss: 0.5929  loss_cls: 0.003177  loss_box_reg: 0.0008854  loss_keypoint: 0.5838  loss_rpn_cls: 0.0006335  loss_rpn_loc: 0.003829    time: 0.3641  last_time: 0.3389  data_time: 0.0505  last_data_time: 0.0573   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:46 d2.utils.events]: \u001b[0m eta: 8:01:01  iter: 1179  total_loss: 0.6855  loss_cls: 0.004774  loss_box_reg: 0.000824  loss_keypoint: 0.6761  loss_rpn_cls: 0.0005746  loss_rpn_loc: 0.003652    time: 0.3635  last_time: 0.3332  data_time: 0.0477  last_data_time: 0.0534   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:53 d2.utils.events]: \u001b[0m eta: 8:00:39  iter: 1199  total_loss: 0.6416  loss_cls: 0.003958  loss_box_reg: 0.0007692  loss_keypoint: 0.6312  loss_rpn_cls: 0.0004704  loss_rpn_loc: 0.003739    time: 0.3630  last_time: 0.3245  data_time: 0.0490  last_data_time: 0.0430   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:07:59 d2.utils.events]: \u001b[0m eta: 8:00:42  iter: 1219  total_loss: 0.6895  loss_cls: 0.002718  loss_box_reg: 0.0008728  loss_keypoint: 0.6824  loss_rpn_cls: 0.0005371  loss_rpn_loc: 0.00381    time: 0.3626  last_time: 0.3183  data_time: 0.0591  last_data_time: 0.0406   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:06 d2.utils.events]: \u001b[0m eta: 8:00:24  iter: 1239  total_loss: 0.7199  loss_cls: 0.002354  loss_box_reg: 0.0009142  loss_keypoint: 0.7129  loss_rpn_cls: 0.0007436  loss_rpn_loc: 0.003819    time: 0.3621  last_time: 0.3266  data_time: 0.0507  last_data_time: 0.0529   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:13 d2.utils.events]: \u001b[0m eta: 8:00:18  iter: 1259  total_loss: 0.7296  loss_cls: 0.002934  loss_box_reg: 0.0009369  loss_keypoint: 0.7222  loss_rpn_cls: 0.0005657  loss_rpn_loc: 0.003824    time: 0.3615  last_time: 0.3237  data_time: 0.0483  last_data_time: 0.0425   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:19 d2.utils.events]: \u001b[0m eta: 8:00:09  iter: 1279  total_loss: 0.6302  loss_cls: 0.001917  loss_box_reg: 0.0008924  loss_keypoint: 0.6228  loss_rpn_cls: 0.0005314  loss_rpn_loc: 0.00368    time: 0.3610  last_time: 0.3424  data_time: 0.0522  last_data_time: 0.0637   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:26 d2.utils.events]: \u001b[0m eta: 8:00:09  iter: 1299  total_loss: 0.6395  loss_cls: 0.001777  loss_box_reg: 0.0008403  loss_keypoint: 0.6335  loss_rpn_cls: 0.0007243  loss_rpn_loc: 0.003833    time: 0.3606  last_time: 0.3476  data_time: 0.0576  last_data_time: 0.0727   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:33 d2.utils.events]: \u001b[0m eta: 8:00:21  iter: 1319  total_loss: 0.7309  loss_cls: 0.002116  loss_box_reg: 0.000783  loss_keypoint: 0.7232  loss_rpn_cls: 0.0006454  loss_rpn_loc: 0.003552    time: 0.3602  last_time: 0.3464  data_time: 0.0499  last_data_time: 0.0602   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:39 d2.utils.events]: \u001b[0m eta: 8:00:37  iter: 1339  total_loss: 0.5706  loss_cls: 0.002774  loss_box_reg: 0.0007435  loss_keypoint: 0.5616  loss_rpn_cls: 0.0005373  loss_rpn_loc: 0.003206    time: 0.3598  last_time: 0.3238  data_time: 0.0495  last_data_time: 0.0400   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:46 d2.utils.events]: \u001b[0m eta: 8:00:13  iter: 1359  total_loss: 0.6664  loss_cls: 0.003189  loss_box_reg: 0.0006393  loss_keypoint: 0.6593  loss_rpn_cls: 0.0004585  loss_rpn_loc: 0.003526    time: 0.3594  last_time: 0.3183  data_time: 0.0510  last_data_time: 0.0416   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:53 d2.utils.events]: \u001b[0m eta: 8:00:01  iter: 1379  total_loss: 0.5156  loss_cls: 0.00197  loss_box_reg: 0.0007163  loss_keypoint: 0.5081  loss_rpn_cls: 0.0004835  loss_rpn_loc: 0.003317    time: 0.3590  last_time: 0.3496  data_time: 0.0529  last_data_time: 0.0697   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:08:59 d2.utils.events]: \u001b[0m eta: 8:00:20  iter: 1399  total_loss: 0.575  loss_cls: 0.003655  loss_box_reg: 0.001037  loss_keypoint: 0.5663  loss_rpn_cls: 0.0005198  loss_rpn_loc: 0.003346    time: 0.3587  last_time: 0.3388  data_time: 0.0583  last_data_time: 0.0596   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:09:06 d2.utils.events]: \u001b[0m eta: 8:00:27  iter: 1419  total_loss: 0.5753  loss_cls: 0.002596  loss_box_reg: 0.0007413  loss_keypoint: 0.5664  loss_rpn_cls: 0.00055  loss_rpn_loc: 0.003399    time: 0.3583  last_time: 0.3277  data_time: 0.0485  last_data_time: 0.0420   lr: 0.002  max_mem: 24522M\n",
      "\u001b[32m[04/01 16:09:13 d2.utils.events]: \u001b[0m eta: 8:00:21  iter: 1439  total_loss: 0.5495  loss_cls: 0.002576  loss_box_reg: 0.0007524  loss_keypoint: 0.5423  loss_rpn_cls: 0.0005625  loss_rpn_loc: 0.003373    time: 0.3579  last_time: 0.3243  data_time: 0.0444  last_data_time: 0.0412   lr: 0.002  max_mem: 24522M\n"
     ]
    }
   ],
   "source": [
    "# Træls Warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "from detectron2.data.datasets import convert_to_coco_json\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import transforms as T\n",
    "\n",
    "# ----------------------------------------\n",
    "# Custom Augmentation Mapper\n",
    "# ----------------------------------------\n",
    "from detectron2.structures import Instances\n",
    "\n",
    "def custom_mapper(dataset_dict):\n",
    "    dataset_dict = dataset_dict.copy()\n",
    "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "\n",
    "    aug = [\n",
    "        T.ResizeShortestEdge(short_edge_length=(640, 720, 768), max_size=1024, sample_style='choice'),\n",
    "        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "        T.RandomBrightness(0.9, 1.1),\n",
    "        T.RandomContrast(0.9, 1.1),\n",
    "    ]\n",
    "    image, transforms = T.apply_transform_gens(aug, image)\n",
    "    image_tensor = torch.as_tensor(image.transpose(2, 0, 1).copy(), dtype=torch.float32)\n",
    "    dataset_dict[\"image\"] = image_tensor\n",
    "\n",
    "    annos = [utils.transform_instance_annotations(\n",
    "        obj, transforms, image.shape[:2],\n",
    "        keypoint_hflip_indices=[0]\n",
    "    ) for obj in dataset_dict[\"annotations\"]]\n",
    "\n",
    "    # 👇 The critical fix\n",
    "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
    "    dataset_dict[\"instances\"] = instances\n",
    "    return dataset_dict\n",
    "# ----------------------------------------\n",
    "# Custom Trainer w/ Eval + Augmentations\n",
    "# ----------------------------------------\n",
    "class CustomTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Config Setup\n",
    "# ----------------------------------------\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"Overbite_Data\",)\n",
    "cfg.DATASETS.TEST = (\"Overbite_Validation\",)\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 8\n",
    "cfg.SOLVER.IMS_PER_BATCH = 16\n",
    "cfg.SOLVER.BASE_LR = 0.002\n",
    "\n",
    "cfg.SOLVER.MAX_ITER = 90000\n",
    "cfg.SOLVER.STEPS = (60000, 80000)\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "\n",
    "cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "cfg.SOLVER.WARMUP_METHOD = \"linear\"\n",
    "cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "\n",
    "cfg.SOLVER.AMP.ENABLED = True\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.KEYPOINT_ON = True\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 1\n",
    "\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.1]\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "\n",
    "cfg.OUTPUT_DIR = \"./output/Overbite_Model\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------\n",
    "# COCO Conversion for Validation Set\n",
    "# ----------------------------------------\n",
    "coco_annotation_path = os.path.join(cfg.OUTPUT_DIR, \"Overbite_Validation_coco_format.json\")\n",
    "convert_to_coco_json(\"Overbite_Validation\", coco_annotation_path)\n",
    "print(f\"Validation set converted to COCO format: {coco_annotation_path}\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Training\n",
    "# ----------------------------------------\n",
    "trainer = CustomTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Final Evaluation\n",
    "# ----------------------------------------\n",
    "val_loader = build_detection_test_loader(cfg, \"Overbite_Validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, COCOEvaluator(\"Overbite_Validation\", cfg, False, output_dir=cfg.OUTPUT_DIR))\n",
    "\n",
    "print(\"Training and validation completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
