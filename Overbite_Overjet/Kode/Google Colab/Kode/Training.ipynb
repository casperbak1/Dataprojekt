{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and import of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install pyyaml\n",
    "import sys, os, distutils.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Detectron 2\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch, detectron2\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Andre pakker\n",
    "\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow\n",
    "import pandas as pd\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import convert_to_coco_json\n",
    "import shutil\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the images from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import subprocess\n",
    "\n",
    "# Get GitHub token\n",
    "token = getpass(\"Enter your GitHub token: \")\n",
    "\n",
    "# Define repository URL\n",
    "repo_url = f\"https://{token}@github.com/casperbak1/Dataprojekt.git\"\n",
    "\n",
    "# Kloner main branch, og kun seneste commit (depth 1)\n",
    "subprocess.run([\"git\", \"clone\", \"--branch\", \"main\", \"--depth\", \"1\", repo_url])\n",
    "\n",
    "repo_path = \"Dataprojekt\"\n",
    "if os.path.exists(repo_path):  # Hvis der findes en GitHub sti \"Dataprojekt\"\n",
    "    subprocess.run([\"git\", \"sparse-checkout\", \"init\", \"--cone\"], cwd=repo_path) # Så klon mappen \"Data/Clean Data/Overbite Data\"\n",
    "    subprocess.run([\"git\", \"sparse-checkout\", \"set\", \"Data/Clean Data/Overbite Data\"], cwd=repo_path)\n",
    "\n",
    "print(\"Repository cloned with only the 'Overbite Data' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise the data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the annotations from the CSV file\n",
    "ANNOTATIONS_FILE = \"Dataprojekt/Data/Clean Data/Overbite Data/Updated_Labels.csv\"\n",
    "annotations_df = pd.read_csv(ANNOTATIONS_FILE)\n",
    "\n",
    "# Sti til dataen\n",
    "DATASET_PATH = \"Dataprojekt/Data/Clean Data/Overbite Data/Annotated Data\"\n",
    "\n",
    "def my_dataset_function():\n",
    "    dataset_dicts = []\n",
    "\n",
    "    # Group annotations by filename (in case multiple keypoints exist for an image)\n",
    "    grouped_annotations = annotations_df.groupby(\"Filename\")\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(DATASET_PATH)):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            record = {}\n",
    "            file_path = os.path.join(DATASET_PATH, filename)\n",
    "\n",
    "            # Read image dimensions\n",
    "            height, width = cv2.imread(file_path).shape[:2]\n",
    "\n",
    "            # Initialize the dataset record\n",
    "            record[\"file_name\"] = file_path\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "\n",
    "            # Default empty annotation list\n",
    "            record[\"annotations\"] = []\n",
    "\n",
    "            # Check if the file has keypoint annotations\n",
    "            if filename in grouped_annotations.groups:\n",
    "                keypoints_list = []\n",
    "                for _, row in grouped_annotations.get_group(filename).iterrows():\n",
    "                    x, y = row[\"X\"], row[\"Y\"]\n",
    "                    keypoints_list.append(x)  # X-coordinate\n",
    "                    keypoints_list.append(y)  # Y-coordinate\n",
    "                    keypoints_list.append(2)  # Visibility flag (0=not visible, 1=occluded, 2=visible)\n",
    "\n",
    "                # Create the annotation entry\n",
    "                annotation = {\n",
    "                    \"bbox\": [0, 0, width, height],  # Dummy bbox covering entire image\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": 0,  # If you have multiple classes (Vi har 1)\n",
    "                    \"keypoints\": keypoints_list,\n",
    "                    \"num_keypoints\": len(keypoints_list) // 3\n",
    "                }\n",
    "                record[\"annotations\"].append(annotation)\n",
    "\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "# Register the dataset\n",
    "DatasetCatalog.register(\"Overbite_Data\", my_dataset_function)\n",
    "MetadataCatalog.get(\"Overbite_Data\").set(\n",
    "    thing_classes=[\"object\"],  # Modify for actual class names (Mangler godt navn)\n",
    "    keypoint_names=[\"keypoint\"],  # Name of keypoints (Mangler endnu bedre navn)\n",
    "    keypoint_flip_map=[]  # Add keypoint flip pairs if needed (Nope)\n",
    ")\n",
    "\n",
    "# Test if it works\n",
    "dataset_dicts = DatasetCatalog.get(\"Overbite_Data\")\n",
    "print(f\"Loaded {len(dataset_dicts)} images with keypoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Funktion til at hente verifikationsdata\n",
    "def my_validation_function():\n",
    "    dataset_dicts = []\n",
    "    DATASET_PATH = \"Dataprojekt/Data/Clean Data/Overbite Data/Annotated Verification Data\"\n",
    "\n",
    "    grouped_annotations = annotations_df.groupby(\"Filename\")\n",
    "\n",
    "    for idx, filename in enumerate(os.listdir(DATASET_PATH)):\n",
    "        if filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            record = {}\n",
    "            file_path = os.path.join(DATASET_PATH, filename)\n",
    "            height, width = cv2.imread(file_path).shape[:2]\n",
    "\n",
    "            record[\"file_name\"] = file_path\n",
    "            record[\"image_id\"] = idx\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "            record[\"annotations\"] = []\n",
    "\n",
    "            if filename in grouped_annotations.groups:\n",
    "                keypoints_list = []\n",
    "                for _, row in grouped_annotations.get_group(filename).iterrows():\n",
    "                    x, y = row[\"X\"], row[\"Y\"]\n",
    "                    keypoints_list.append(x)\n",
    "                    keypoints_list.append(y)\n",
    "                    keypoints_list.append(2)  # Visibility flag\n",
    "\n",
    "                annotation = {\n",
    "                    \"bbox\": [0, 0, width, height],  # Dummy bbox\n",
    "                    \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                    \"category_id\": 0,\n",
    "                    \"keypoints\": keypoints_list,\n",
    "                    \"num_keypoints\": len(keypoints_list) // 3\n",
    "                }\n",
    "                record[\"annotations\"].append(annotation)\n",
    "\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "# Registrér valideringsdatasæt\n",
    "DatasetCatalog.register(\"Overbite_Validation\", my_validation_function)\n",
    "MetadataCatalog.get(\"Overbite_Validation\").set(\n",
    "    thing_classes=[\"object\"],\n",
    "    keypoint_names=[\"keypoint\"],\n",
    "    keypoint_flip_map=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import convert_to_coco_json\n",
    "\n",
    "# Create the configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))  # Load Keypoint-RCNN config\n",
    "cfg.DATASETS.TRAIN = (\"Overbite_Data\",)  # Training dataset\n",
    "cfg.DATASETS.TEST = (\"Overbite_Validation\",)  # Validation dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\")  # Pretrained weights\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # Batch size (Antal billeder per batch) (Større skulle gerne give god træning, men vi har begrænset GPU i \"Colab\")\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # Learning rate\n",
    "cfg.SOLVER.MAX_ITER = 8000  # Antal træningsiterationer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # Antal \"Regions Of Interest\" per billede\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Antal klasser\n",
    "cfg.MODEL.KEYPOINT_ON = True  # Enable keypoint detection\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 1  # Antal keypoints\n",
    "\n",
    "\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = [0.1] # Set OKS sigma for 1 keypoint (Default is 17 for COCO)\n",
    "\n",
    "# Automatisk validering hver 500. iteration\n",
    "cfg.TEST.EVAL_PERIOD = 500\n",
    "\n",
    "# Output directory for modellen\n",
    "cfg.OUTPUT_DIR = \"./output/Overbite_Model\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Konverter valideringsettet til COCO JSON format\n",
    "coco_annotation_path = os.path.join(cfg.OUTPUT_DIR, \"Overbite_Validation_coco_format.json\")\n",
    "convert_to_coco_json(\"Overbite_Validation\", coco_annotation_path)\n",
    "print(f\"Validation set converted to COCO format: {coco_annotation_path}\")\n",
    "\n",
    "# Inkluder evaluatoren\n",
    "class TrainerWithEval(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name):\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "\n",
    "# Træn modellen med evaluering\n",
    "trainer = TrainerWithEval(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# Kør en sidste evaluering på valideringsdataen\n",
    "val_loader = build_detection_test_loader(cfg, \"Overbite_Validation\")\n",
    "inference_on_dataset(trainer.model, val_loader, COCOEvaluator(\"Overbite_Validation\", cfg, False, output_dir=cfg.OUTPUT_DIR))\n",
    "\n",
    "print(\"Training and validation completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
