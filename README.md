# Overbidsklassificering â€” Automatisering og standardisering  
*Et maskinlÃ¦ringsprojekt til klassificering af overbid ud fra 3D-scanninger og billeder.*

---

## Indholdsfortegnelse 

- [Hurtigt Overblik](#hurtigt-overblik-over-filstruktur )
  - [Mappestruktur](#Mappestruktur)
- [Projektbeskrivelse](#projektbeskrivelse)
  - [Abstract](#abstract)
  - [Introduktion](#introduktion)
  - [Data og Databehandling](#data-og-databehandling)
    - [Databehandling](#databehandling)
  - [RCNN-netvÃ¦rk](#Keypoints-R-CNN-netvÃ¦rk)
  - [Pixel-matrix](#pixel-matrix)
  - [Evalueringsmetoder](#evalueringsmetoder)
  - [Resultater](#resultater)
  - [Pipeline](#pipeline)

---

## Hurtigt overblik over filstruktur

- **Data:** Indeholder alle rÃ¥ og forarbejdede CSV- og PNG-filer, primÃ¦rt brugt til trÃ¦ning og test.
- **Overbite:** Indeholder kode, scripts, outputs og forskellige versioner brugt til overbite-klassificering.
- **Pipeline:** Viser hele workflowet fra 3D .PLY-fil til keypoint-placering.

---

## Mappestruktur
<details open>
  <summary>ğŸ“ Dataprojekt/</summary>
  <ul>
    <li>ğŸ“„ .gitignore</li>
    <li>ğŸ“„ Model.txt</li>
    <li>ğŸ“„ README.md</li>
    <li>
      <details>
        <summary>ğŸ“ Data/</summary>
        <ul>
          <li>ğŸ“„ pixel_flip_formula.png</li>
          <li>ğŸ“„ Splitting_and_flipping_of_images.ipynb</li>
          <li>
            <details>
              <summary>ğŸ“ Clean Data/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Bolton Data/</summary>
                    <ul>
                      <li>ğŸ“„ Example_lower_middle.png</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite Data/</summary>
                    <ul>
                      <li>ğŸ“„ Updated_Labels.csv</li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Data Pairs/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Test data/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Verication data/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Unannotated Data Pairs/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Figurer/</summary>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Raw Data/</summary>
              <ul>
                <li>ğŸ“„ 2024-04-08 Test data for overbite classification.xlsx</li>
                <li>ğŸ“„ 2025-05-08 TRANSLATE_KEY(1).xlsx</li>
                <li>ğŸ“„ Definitions of columns.docx</li>
                <li>ğŸ“„ Labels as of 19-02-2025 (Sample images).csv</li>
                <li>ğŸ“„ Labels as of 28-02-2025 (FINAL - for now).csv</li>
                <li>
                  <details>
                    <summary>ğŸ“ Sample images/</summary>
                    <ul>
                      <li>ğŸ“„ Example_lower_combined.png</li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
    <li>
      <details>
        <summary>ğŸ“ Overbite/</summary>
        <ul>
          <li>
            <details>
              <summary>ğŸ“ Kode/</summary>
              <ul>
                <li>ğŸ“„ Overbite.ipynb</li>
                <li>ğŸ“„ Pixel_Matrix_Optimizer.ipynb</li>
                <li>ğŸ“„ Test_Model.ipynb</li>
                <li>ğŸ“„ Train_Model.ipynb</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Other Versions (Overbite)/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Kode/</summary>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Output/</summary>
                    <ul>
                      <li>
                        <details>
                          <summary>ğŸ“ Modeller/</summary>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Overbite Detection/</summary>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Pixel Matrix/</summary>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Output/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Keypoint Placement/</summary>
                    <ul>
                      <li>ğŸ“„ KP_Placement.csv</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Modeller/</summary>
                    <ul>
                      <li>ğŸ“„ Model.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite Detection/</summary>
                    <ul>
                      <li>ğŸ“„ All_FALSE_Classification_Rows.csv</li>
                      <li>ğŸ“„ Overbite_Classification9.csv</li>
                      <li>ğŸ“„ Results.ipynb</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Pixel Matrix/</summary>
                    <ul>
                      <li>ğŸ“„ KP_Refinement.csv</li>
                      <li>ğŸ“„ KP_Refinement_Distance.csv</li>
                      <li>
                        <details>
                          <summary>ğŸ“ Image Output/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.html</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
    <li>
      <details>
        <summary>ğŸ“ Pipeline/</summary>
        <ul>
          <li>ğŸ“„ pipeline.ipynb</li>
          <li>ğŸ“„ README.txt</li>
          <li>
            <details>
              <summary>ğŸ“ docker_detectron2_env/</summary>
              <ul>
                <li>ğŸ“„ Dockerfile_pytorch3d_jupyter</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ output/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite_Model/</summary>
                    </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Pipeline_code/</summary>
              <ul>
                <li>ğŸ“„ Opdeling_og_flip_af_billeder.py</li>
                <li>ğŸ“„ Overbite.py</li>
                <li>ğŸ“„ Pixelmatrix.py</li>
                <li>ğŸ“„ Ply_To_Image.py</li>
                <li>ğŸ“„ Run_model.py</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Pipeline_data/</summary>
              <ul>
                <li>ğŸ“„ patient_level_summary4.csv</li>
                <li>ğŸ“„ Predicted_keypoints.csv</li>
                <li>
                  <details>
                    <summary>ğŸ“ Clean Data/</summary>
                    <ul>
                      <li>
                        <details>
                          <summary>ğŸ“ Overbite Data/</summary>
                          <ul>
                            <li>ğŸ“„ Info.txt</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Model/</summary>
                    <ul>
                      <li>ğŸ“„ Info.txt</li>
                      <li>ğŸ“„ Model.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Output_after_pixel_matrix/</summary>
                    <ul>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Ply Files/</summary>
                    <ul>
                      <li>ğŸ“„ Brunatest LowerJawScan.ply</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan.ply</li>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Raw_data/</summary>
                    <ul>
                      <li>ğŸ“„ Brunatest LowerJawScan_0.png</li>
                      <li>ğŸ“„ Brunatest LowerJawScan_1.png</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan_0.png</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan_1.png</li>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
  </ul>
</details>

---

# Projektbeskrivelse


---

## Abstract

Dette projekt prÃ¦senterer en automatiseret lÃ¸sning til klassificering af overbid ud fra todimensionelle billeder baseret pÃ¥ tilhÃ¸rende 3D-scanninger (IOS).
Ved at anvende Keypoint R-CNN med en ResNeXt-101 Ã— 32d FPN-backbone trÃ¦nes modellen til at lokalisere prÃ¦cise keypoints i intraorale billeder. Efter den fÃ¸rste keypoint-forudsigelse finjusteres positionerne ved hjÃ¦lp af en pixel-matrix-sÃ¸gning, der vÃ¦lger den lyseste pixel inden for et bestemt omrÃ¥de, hvilket reducerer fejl i punktplacering fra i gennemsnit 0,095 mm til 0,037 mm pÃ¥ y-aksen. Evaluering med Mean Radial Error (MRE) og Success Detection Rate (SDR) viser, at pixel-matrix justeringen halverer MRE fra omkring 0,22 mm til 0,12 mm og forbedrer SDR for nÃ¦sten alle thresholds (0,5 mm, 1 mm, 2 mm) til 89 %, 96 % og 99 % henholdsvis. Patientniveau-klassificeringen af overbid i fem klasser (Aâ€“E) opnÃ¥r en gennemsnitlig nÃ¸jagtighed pÃ¥ 95,34 % og en kvadratisk vÃ¦gtet Cohenâ€™s kappa pÃ¥ 0,989 pÃ¥ simulerede testdata. Endvidere er der udviklet en pipeline, der tager .PLY filer som input, og derigennem identificerer de punkter, hvorfra overbiddet kan mÃ¥les. 


---

## Introduktion

I tandlÃ¦gepraksis findes der ikke Ã©n standardiseret metode til mÃ¥ling af overbid. Nogle anvender Ã¸jemÃ¥l, andre lineal, rÃ¸ntgenbilleder eller 3D-scanninger. Alle metoder har fordele og ulemper, men ofte er der en afvejning mellem prÃ¦cision og tidsforbrug.

<h3>Overbid</h3>

Overbid refererer til den vertikale afstand mellem de Ã¸verste og nederste fortÃ¦nder, mÃ¥lt fra spidsen af overkÃ¦bens fortÃ¦nder til spidsen af underkÃ¦bens. Et for stort overbid indikerer, at de Ã¸verste fortÃ¦nder dÃ¦kker en unormalt stor del af de nederste, hvilket kan have funktionelle og Ã¦stetiske konsekvenser.
Ved at mÃ¥le denne afstand kan man klassificere graden af overbid hos en patient og pÃ¥ den baggrund vurdere, om der er behov for behandling.
Et normalt overbid defineres typisk som en vertikal afstand pÃ¥ mellem 2 og 4 mm.
I dette projekt er fÃ¸lgende klassifikationer blevet anvendt som udgangspunkt for evaluering og test af modellen. Disse klasser er udarbejdet af vores vejledere:

<h4>Overbidsklasser</h4>

<table>
  <tr>
    <!-- First column: the table content as plain HTML rows -->
    <td>
      <table border="1">
        <tr><th>Klasse</th><th>Vertikal afstand</th></tr>
        <tr><td>A</td><td>&lt; 1 mm</td></tr>
        <tr><td>B</td><td>1â€“2 mm</td></tr>
        <tr><td>C</td><td>2â€“3 mm</td></tr>
        <tr><td>D</td><td>3â€“4 mm</td></tr>
        <tr><td>E</td><td>&gt; 4 mm</td></tr>
      </table>
    </td>
    <!-- Second column: the image -->
    <td style="padding-left: 30px;">
      <img src="Data/Figurer/Overbite_Figure.png" alt="Illustration af overbid" width="400" height="280"/>
    </td>
  </tr>
</table>

> https://www.mdpi.com/2075-4426/13/10/1472 Figur 1 (03/06/2025)


**Dette projekt har to hovedfokusomrÃ¥der:**

1. Udvikling og trÃ¦ning af maskinlÃ¦ringsmodeller til prÃ¦cis overbidsklassificering.
2. Udarbejdelse af en pipeline, der gÃ¥r fra 3D-filer (.PLY) til keypoint-markering, der muliggÃ¸r automatisk mÃ¥ling.

**BemÃ¦rk:** Resten af denne projektbeskrivelse omhandler kun punkt 1. Pipeline beskrives separat i sektionen "Pipeline".

Projektet bygger pÃ¥ et offentligt datasÃ¦t bestÃ¥ende af 1.351 tredimensionelle intraorale scanninger. Til dette projekt er 3D-scanningerne konverteret til todimensionelle billeder, hvilket gÃ¸r det muligt at anvende gÃ¦ngse deep-learning-metoder til billedanalyse. 

Som model anvender vi Keypoint R-CNN, en videreudvikling af Mask R-CNN, der er designet til at finde prÃ¦cise keypoints i billeder. Ved at kombinere regionsÂ­forslag med punktdetektion gÃ¸r modellen det muligt at identificere nÃ¸jagtige punkter pÃ¥ tÃ¦nder, hvilket er afgÃ¸rende for vores opgave, da overbid mÃ¥les som den vertikale afstand mellem to prÃ¦cise punkter pÃ¥ fortÃ¦nderne.

---

## Data og Databehandling

### Databehandling

Projektet startede med **1351 kombinerede billeder** (3 vinkler pr. patient):

<img src="Data/Figurer/00OMSZGW_lower_combined.png" width="800" height="250"/>

> *Billede af underkÃ¦be fra 3 vinkler*

Til billederne var der **1166 annoteringer**, eksempelvis:

| Filename                       | X1  | Y1  | X2   | Y2  |
|---------------------------------|-----|-----|------|-----|
| 00OMSZGW_lower_combined.png     | 777 | 492 | 2310 | 487 |

#### MÃ¥l for databehandling:

1. Split kombinerede billeder til tre separate billeder (left, middle, right)
2. Fordel billeder i mapper: **Bolton Data** og **Overbite Data**
3. Ensret orientering og koordinationer i "Overbite Data" (ved transponering)
4. GruppÃ©r data som billed-par
5. Del data i trÃ¦ning, test, verifikation, og uannoteret data

#### Step 1: Opdeling af billeder

<table>
  <tr>
    <td><img src="Data/Figurer/Lower_Right.png" width="250" height="180"/></td>
    <td><img src="Data/Figurer/Lower_Middle.png" width="250" height="180"/></td>
    <td><img src="Data/Figurer/Lower_Left.png" width="250" height="180"/></td>
  </tr>
</table>


#### Step 2: Sortering

- **Left/Right** billeder â†’ *Overbite Data*
- **Middle** billede â†’ *Bolton Data*

#### Step 3: Ensretning

"Right" billedet transponeres (flippes), sÃ¥ orienteringen matcher "Left" billedet, og koordinater justeres:\

<table>
  <tr>
    <td><img src="Data/Figurer/Lower_Right.png" width="400" height="400"/></td>
    <td><img src="Data/Figurer/Lower_Left_Flipped.png" width="400" height="400"/></td>
  </tr>
</table>

> Keypoint/Annoteringen er markeret med rÃ¸d pÃ¥ de 2 billeder

Koordinatet justeres sÃ¥ med fÃ¸lgende formel:
`x' = w - 1 - x`\
Her er `x = X2 = 2310` fra originalen, w = 3072 fra originalen, og derfor er det nye x beregnet til:\
`x' = 3072 - 1 - 2310 = 761`

Og den nye CSV vil derfor blive til:

| Filename                   | X1  | Y1  |
|----------------------------|-----|-----|
| 00OMSZGW_lower_left.png   | 777 | 492 |
| 00OMSZGW_lower_right.png    | 761 | 487 |


#### Step 4 + 5: Par og grupper

- Et *par* = 4 billeder med koordinater (left/right, upper/lower)
- UfuldstÃ¦ndige par â†’ *Unannotated Data Pairs*
- Resten inddeles i:
    - *Annotated Data Pairs* (trÃ¦ning)
    - *Annotated Verication data* (verifikation under trÃ¦ning)
    - *Annotated Test data* (test efter trÃ¦ning)

> Outliers og fejl rettes i CSV efter databehandling.  
> Alt databehandling findes i `Splitting_and_flipping_of_images.ipynb`.


---

Oversigt over fordelingen af billeder:

| Folder                               | Image count | Patient count | Annotated |
|---------------------------------------|-------------|-------------|-------------|
| Bolton Data                          | 1351        | 675 | No |
| Overbite Data                        | 2702        | 675 | NaN |
| Overbite Data/Annotated Data Pairs   | 1580        | 395 | Yes |
| Overbite Data/Annotated Verication data | 100      | 25 | Yes |
| Overbite Data/Annotated Test data    | 300         | 75 | Yes |
| Overbite Data/Unannotated Data Pairs | 722         | 180 | NaN|

Efter dataen er opdelt i mapper mangler vi kun 2 ting inden trÃ¦ningen kan pÃ¥begynde:

1. OpsÃ¦tte en bounding boks
2. Konvertere annoteringer til "COCO JSON" format

Vi opsÃ¦tter en bounding boks for at hjÃ¦lpe med at lokalisere objektet af interesse. Det hjÃ¦lper modellen med at finde det omrÃ¥de den skal fokusere pÃ¥, for at lave forudsigelser. Dette gÃ¸r modellen mere prÃ¦cis og effektiv.

Annoteringerne skal til sidst konverteres til et format som kan lÃ¦ses af modellen.

Modellen modtager altsÃ¥ som input til trÃ¦ning 1580 billeder af fÃ¸lgende format:

<img src="Data/Figurer/Data_For_Training.png" width="600" height="600"/>

> *Keypoint markeret med rÃ¸d, og bounding box markeret med grÃ¥*

Og annoteringen:

```json
{
  "images": [
    {
      "file_name": "Dataprojekt/Data/Clean Data/Overbite Data/Annotated Data Pairs/00OMSZGW_lower_left.png",
      "height": 1024,
      "width": 1024,
      "id": 0
    }
  ],
  "annotations": [
    {
      "bbox": [740, 492, 62, 125],
      "bbox_mode": 1,
      "category_id": 0,
      "keypoints": [777, 492, 2],
      "num_keypoints": 1,
      "image_id": 0,
      "id": 0
    }
  ],
  "categories": [
    {
      "id": 0,
      "name": "tooth",
      "keypoints": ["apex"],
      "skeleton": []
    }
  ]
}
```

Modellen er altsÃ¥ nu klar til at blive trÃ¦net.


---

## Keypoints R-CNN-netvÃ¦rk

I vores arbejde har vi trÃ¦net en model ved hjÃ¦lp af en forudtrÃ¦net model fra Detectron2-biblioteket. Den specifikke model, vi har anvendt, er **"keypoint_rcnn_X_101_32x8d_FPN_3x"**. 

* **Modeltype:** Keypoint R-CNN. Dette indikerer, at modellen er designet til at identificere specifikke nÃ¸glepunkter (keypoints) pÃ¥ objekter.
* **Backbone-arkitektur:** X_101_32x8d_FPN. Dette er kernen i modellen og bestÃ¥r af:
    * **ResNeXt-101 (X_101_32x8d):** En ResNeXt-arkitektur med 101 lag. "32x8d" specificerer, at netvÃ¦rket anvender 32 parallelle grupperede convolutions, hvor hver gruppe har en bredde pÃ¥ 8. Dette forbedrer modellens evne til at lÃ¦re komplekse mÃ¸nstre.
    * **FPN (Feature Pyramid Network):** GÃ¸r det muligt for modellen at lÃ¦re og detektere objekter pÃ¥ tvÃ¦rs af forskellige skalaer ved at udnytte feature-maps fra flere niveauer i netvÃ¦rket.

### TrÃ¦ningsprocessen

TrÃ¦ningen kan grundlÃ¦ggende opdeles i fÃ¸lgende faser:

1.  **Inputdata:**
    * Vi starter med billeder i en oplÃ¸sning pÃ¥ 1024x1024 pixels.
    * Hvert billede er forsynet med et "ground truth" keypoint, hvilket er den korrekte, manuelt markerede placering af det Ã¸nskede nÃ¸glepunkt.

2.  **Data Augmentering:**
    * GÃ¸r modellen mere robust ved at udsÃ¦tte billederne for forskellige augmenteringsteknikker.
    * Inkluderer variationer i billedstÃ¸rrelse, spejlinger (horisontalt/vertikalt), samt justeringer af lys og kontrast.
    * TilhÃ¸rende annoteringer (keypoints) justeres automatisk for at matche de augmenterede billeder (f.eks. flyttes keypointet med, hvis billedet spejles).

3.  **Batch-indlÃ¦sning:**
    * Billederne indlÃ¦ses i modellen i "batches" (grupper af billeder). Vi har anvendt en batch-stÃ¸rrelse pÃ¥ 16.
    * TrÃ¦ning med batches frem for individuelle billeder har to primÃ¦re fordele:
        * **Hurtigere trÃ¦ning:** Parallel processering af flere billeder er mere effektivt.
        * **Mere stabil trÃ¦ning:** Modellens vÃ¦gtjusteringer baseres pÃ¥ gennemsnittet af fejlen for hele batchen. Dette reducerer risikoen for, at enkelte afvigende billeder (outliers) fÃ¥r stor indflydelse pÃ¥ lÃ¦ringen, hvilket ville vÃ¦re tilfÃ¦ldet ved opdatering baseret pÃ¥ Ã©t billede ad gangen.

4.  **Feature Extraction med ResNeXt-101:**
    * De augmenterede billeder (i batches) fÃ¸res nu igennem ResNeXt-101 backbone-arkitekturen.
    * De 101 lag i netvÃ¦rket udfÃ¸rer en serie af convolution-operationer.
    * Hver convolution er yderligere opdelt i 32 grupper, som hver isÃ¦r specialiserer sig i at finde bestemte mÃ¸nstre i billeddataene.
    * Resultatet af denne proces er en samling af "feature maps". Hver pixel i et feature map reprÃ¦senterer styrken eller tilstedevÃ¦relsen af et specifikt mÃ¸nster i det oprindelige billede.

5.  **Feature Pyramid Network (FPN):**
    * FPN-komponenten tager de feature maps, der er genereret af ResNeXt-101.
    * Styrken ved FPN er, at den kombinerer feature maps fra forskellige dybder (lag) i netvÃ¦rket.
        * Tidlige lag fanger typisk simple, lav-niveau mÃ¸nstre (kanter, teksturer).
        * Sene lag fanger mere komplekse, hÃ¸j-niveau mÃ¸nstre og helheder (objektdele, hele objekter).
    * Ved at samle disse informationer kan modellen bedre finde objekter og keypoints uanset deres stÃ¸rrelse i billedet.

6.  **Region Proposal Network (RPN):**
    * RPN modtager de kombinerede feature maps fra FPN.
    * Dens opgave er at generere tusindvis af "region proposals" â€“ smÃ¥ afgrÃ¦nsede bokse af forskellige stÃ¸rrelser og former, der dÃ¦kker omrÃ¥der i billedet, hvor der potentielt kunne vÃ¦re et relevant objekt.
    * For hver af disse foreslÃ¥ede bokse beregner RPN en score, der angiver sandsynligheden for, at boksen rent faktisk indeholder et objekt af interesse.
    * De mest lovende forslag (dem med hÃ¸jest score) sendes videre til det nÃ¦ste trin.

7.  **Keypoint Lokalisering:**
    * For hver af de udvalgte region proposals fra RPN genererer modellen et "heatmap", hvor hver pixelvÃ¦rdi reprÃ¦senterer sandsynligheden for, at det sÃ¸gte keypoint befinder sig netop pÃ¥ dÃ©n pixelposition inden for den foreslÃ¥ede region.
    * Disse sandsynligheder beregnes ved hjÃ¦lp af en softmax-funktion. Softmax sikrer, at summen af alle sandsynlighederne i et heatmap er lig med 1, hvilket gÃ¸r det muligt at tolke vÃ¦rdierne som en sandsynlighedsfordeling.
    * Til sidst identificerer modellen den pixel i heatmappet, der har den hÃ¸jeste sandsynlighedsvÃ¦rdi. Denne pixelposition betragtes som modellens endelige bud pÃ¥ placeringen af keypointet.

8.  **Loss, Backpropagation og Optimering:**
    NÃ¥r modellen har afgivet sit bud pÃ¥ keypointets placering (via det genererede heatmap), er nÃ¦ste skridt at evaluere, hvor prÃ¦cist dette bud er, og derefter justere modellen for at forbedre fremtidige forudsigelser. Denne proces involverer tre centrale elementer:

    * **Loss-beregning:**
        FÃ¸rst beregnes modellens fejl. Dette gÃ¸res ved at sammenligne modellens **forudsagte heatmap** med det **ground truth heatmap**. En **loss-funktion** beregner en score, der angiver, hvor stor forskellen er mellem forudsigelsen og sandheden. For denne opgave anvendes fÃ¸lgende loss-funktioner:

| Modul                     | Loss-funktion                                     |
|---------------------------|-------------------------------|
| **RPN - Region Proposal** | Binary Cross Entropy (BCE)                        |
|                           |                               | Smooth L1                                         |
| **ROI Box Head** | Cross Entropy (Softmax)                           |
|                           |                               | Smooth L1                                         |
| **ROI Keypoint Head** | Binary Cross Entropy (sigmoid pr. keypoint pixel) |
    
    * **Backpropagation:**
        NÃ¥r fejlen (loss) er beregnet, skal vi finde ud af, hvordan hver enkelt vÃ¦gt (parameter) i netvÃ¦rket har bidraget til denne fejl. Det er her, **backpropagation** kommer ind i billedet.
        * Processen starter med den beregnede loss-vÃ¦rdi.
        * Ved hjÃ¦lp af **kÃ¦dereglen for differentiering** beregnes gradienten af loss-funktionen med hensyn til hver vÃ¦gt i netvÃ¦rket. Gradienten fortÃ¦ller os, hvor meget loss-vÃ¦rdien ville Ã¦ndre sig, hvis vi Ã¦ndrede den pÃ¥gÃ¦ldende vÃ¦gt en lille smule â€“ altsÃ¥ vÃ¦gtens "ansvar" for fejlen.
        * Denne beregning foregÃ¥r baglÃ¦ns gennem netvÃ¦rket, lag for lag, fra outputlaget tilbage mod inputlaget.

    * **Optimering:**
        Med gradienterne beregnet via backpropagation ved vi nu, i hvilken "retning" hver vÃ¦gt skal justeres for at reducere fejlen. En **optimeringsalgoritme** (f.eks. Adam, SGD) bruger disse gradienter til at opdatere modellens vÃ¦gte.
        * VÃ¦gtene justeres typisk en lille smule i modsat retning af deres gradient. MÃ¥let er at tage et lille skridt i den retning, der minimerer loss-funktionen. StÃ¸rrelsen af dette skridt styres af hyperparameteren "learning rate".

    * **Iteration og Konvergens:**
        Hele denne cyklus â€“ data gennem modellen, loss-beregning, backpropagation, og vÃ¦gtjustering â€“ gentages for hver **batch** af billeder. Ã‰n fuld gennemgang af hele trÃ¦ningsdatasÃ¦ttet kaldes en **epoch**.
        Modellen bliver gradvist bedre (dvs. loss-vÃ¦rdien falder), efterhÃ¥nden som dens vÃ¦gte finjusteres. TrÃ¦ningen stoppes, nÃ¥r modellen **konvergerer**, hvilket betyder, at dens ydeevne pÃ¥ et separat valideringsdatasÃ¦t ikke lÃ¦ngere forbedres signifikant, eller nÃ¥r et forudbestemt antal epochs er nÃ¥et. For den valgte model stoppede vi trÃ¦ningen efter 152 epochs.


## Pixel-matrix

Efter modellen har placeret keypoints pÃ¥ billederne, finjusteres disse positioner ved hjÃ¦lp af pixelsÃ¸gning. Dette sker ved at definere en matrix (af forudbestemt stÃ¸rrelse) omkring det keypoint, som modellen har forudsagt. Inden for denne matrix identificeres den pixel, der ligger hÃ¸jest og er lysest, og denne pixel vÃ¦lges herefter som det nye, justerede keypoint. Hvis der findes to pixels med samme vÃ¦rdi, bliver den pixel, der ligger lÃ¦ngst til venstre, valgt. Dette valg bygger pÃ¥ hvordan det sande keypoint bliver annoteret. 
FormÃ¥let med denne proces er at sikre, at keypointet placeres sÃ¥ prÃ¦cist som muligt ud fra det forudsagte keypoint fra modellen, hvilket forbedrer den samlede nÃ¸jagtighed og robusthed af keypoint placeringen.

Figuren viser fordelingen af fejl for placeringen af keypoints efter pixelsÃ¸gningen er implementeret.

<img src="Data/Figurer/Histogram_efter_pixelmatrix.png" width="900" height="400"/> 

## Evalueringsmetoder

Til evaluering af modellen har vi benyttet os af tre metoder:

1. **Mean radial error (MRE)**
2. **Succesful detection rate (SDR)**
3. **Weighted Cohenâ€™s kappa**

De er defineret som fÃ¸lger:

**Mean radial error**:

$$
MRE = \frac{\sum_{i=1}^N R_i}{N}
$$

Her er \$N\$ antallet af predikterede punkter, og \$R\_i\$ er den euklidiske afstand mellem ground truth og modellens punkt.

MRE giver os indsigt i, hvor tÃ¦t modellens forudsigelser i gennemsnit ligger pÃ¥ det korrekte punkt (ground truth). Jo lavere MRE, desto mere prÃ¦cis er modellens gennemsnitlige punktplacering.

**Succesful detection rate**:

$$
SDR = \frac{K}{N} \cdot 100
$$

Her er \$N\$ antallet af predikterede punkter, og \$K\$ er antallet af korrekt placerede punkter indenfor et tilladt "fejl"-interval. Vi har brugt intervaller, der tillader 0.5, 1 og 2 mm afstand i forhold til ground truth, hvor vi benytter den euclidiske afstnd.

SDR viser, hvor stor en andel af modellens forudsigelser der ligger indenfor et givent toleranceniveau fra ground truth (fx 0.5, 1 eller 2 mm). Det er sÃ¦rligt nyttigt, hvis man vil vide, hvor ofte modellen rammer â€œtilstrÃ¦kkeligt tÃ¦tâ€ pÃ¥ det korrekte punkt, givet at man accepterer en vis fejlmargin.

**Weighted Cohenâ€™s kappa** (\$\kappa\_w\$) bruges til at mÃ¥le, hvor god overensstemmelse der er mellem modellens klassifikation og den sande (ekspert-annoterede) klasse, hvor tilfÃ¦ldig overensstemmelse er korrigeret for:

$$
\kappa_w = 1 - \frac{\sum_{i,j} w_{i,j} O_{i,j}}{\sum_{i,j} w_{i,j} E_{i,j}}
$$

Her er \$O\_{i,j}\$ antallet af observationer, hvor annoteringen var klasse \$i\$ og modellen valgte klasse \$j\$, og \$E\_{i,j}\$ er den forventede andel af sÃ¥danne tilfÃ¦lde udregnet ved (rÃ¦kke*kolonne/total).

VÃ¦gtene \$w\_{i,j}\$ bruges til at straffe stÃ¸rre fejl hÃ¥rdere. Ved **kvadratisk vÃ¦gtning** (â€œquadratic weightsâ€) beregnes vÃ¦gten som:

$$
w_{i,j} = \left( \frac{i - j}{k - 1} \right)^2
$$

hvor \$k\$ er antallet af klasser (her 5 for â€œAâ€â€“â€œEâ€).

Det vil sige, at hvis modellen forveksler â€œAâ€ og â€œEâ€, tÃ¦ller det som en stÃ¸rre fejl end hvis den forveksler â€œAâ€ og â€œBâ€.

---

## Resultater
Projektets resultater bestÃ¥r primÃ¦rt af:

* CSV-filer med forudsigelser og mÃ¥linger
* Visualiseringer i form af histogrammer
* Billeder med keypoints placeret af modellen

### Modeller og gemte filer

Der er blevet trÃ¦net en rÃ¦kke modeller, som er placeret i mappen `Overbite/Other Versions (Overbite)/`.
Ã‰n af modellerne er udvalgt som den endelige, men flere af de Ã¸vrige modeller prÃ¦sterer nÃ¦sten lige sÃ¥ godt.

Ved afslutningen af trÃ¦ningen gemmes den valgte model som en `.pth`-fil, som anvendes sammen med biblioteket Detectron2.
Da filen overstiger GitHubs uploadgrÃ¦nse, er den ikke inkluderet direkte i repositoryet. Der findes dog et link til filen i den relevante mappe.

### Keypoint-placering

I mappen `Overbite/Output/Keypoint Placement/` ligger filen `KP_Placement.csv`.
Denne indeholder bÃ¥de de sande (ground truth) og de forudsagte keypoints samt den euklidiske distance mellem disse mÃ¥lt i pixels og millimeter.

Eksempel pÃ¥ indhold:

| Filename              | X\_True | Y\_True | X\_Model | Y\_Model | Euc\_dist | mm\_dist |
| --------------------- | ------- | ------- | -------- | -------- | --------- | -------- |
| 013FHA7K\_lower\_left | 844     | 369     | 842.22   | 368.63   | 1.82      | 0.15     |

### Fordeling over afstanden

Nedenfor vises histogrammer over de euklidiske afstande mellem modelens forudsigelser og de sande punkter (ground truth).

<img src="Data/Figurer/Histogram_0_6mm.png" width="600" height="450"/>

> Uden outliers > 0.6 mm. 276/300 Resultater.

<img src="Data/Figurer/Histogram_1_mm.png" width="600" height="450"/>

> Uden outliers > 1 mm. 285/300 Resultater.

<img src="Data/Figurer/Histogram_3_5mm.png" width="600" height="450"/>

> Uden outliers > 3.5 mm. 300/300 Resultater.

### Statistik for y-vÃ¦rdier

Eftersom overbid mÃ¥les ud fra de vertikale afstande mellem fortÃ¦nderne, har vi fundet det relevant at udarbejde statistik for y-vÃ¦rdierne:

|Statistic | Value (pixels)| Value (mm)|
|------------------------|------|-----|
|Mean absolute difference|	1.18|	0.095|
|Mean difference|	0.53|	0.043|
|Median difference|	0.43|	0.035|
|Min difference|	-9.41|	-0.75|
|Max difference|	24.08|	1.93|

Vores model rammer i gennemsnit 1.18 pixels eller 0.095 mm fra den korrekte y-vÃ¦rdi. Den gennemsnitlige difference pÃ¥ 0.53 pixels (0.043 mm) indikerer, at modellen har en tendens til at placere keypointet lidt over den sande vÃ¦rdi. BÃ¥de middel- og medianvÃ¦rdierne er smÃ¥, hvilket tyder pÃ¥ en god prÃ¦cision for langt de fleste punkter. Dog viser minimums- og maksimumsvÃ¦rdierne, at der eksisterer enkelte outliers, hvor modellen afviger markant fra det korrekte resultat.

### Resultater efter pixel-matrix-sÃ¸gning

Efter at have anvendt pixel-matrix-sÃ¸gning opnÃ¥s justerede keypoints. Afstanden mellem disse raffinerede keypoints og de sande vÃ¦rdier er mÃ¥lt i bÃ¥de pixels og millimeter og gemt i KP_Refinement_Distance.csv:

| Filename | X_Model  | Y_Model  | X_Refined  | Y_Refined | X_True | Y_True | Refined_Pixel_Dist| Refined_mm_Dist|
|----------------------------|-----|-----|-----|-----|-----|-----|-----|-----|
| 013FHA7K_lower_left   | 842| 368|843 | 369 | 844 |369 | 1 | 0.08 |


Nedenfor ses et histogram over fordelingen af den euklidiske afstand mellem ground truth og de raffinerede keypoints. Det fremgÃ¥r tydeligt, at fejlen er reduceret markant i forhold til modellens oprindelige forudsigelser.

<img src="Data/Figurer/Histogram_efter_pixelmatrix.png" width="900" height="400"/> 

### Statistik for y-vÃ¦rdier efter pixel-matrix-sÃ¸gning

Efter anvendelse af pixel-matrix-sÃ¸gning er der udarbejdet fÃ¸lgende statistik for forskellene i y-koordinater mellem de justerede keypoints og de korrekte (ground truth) vÃ¦rdier:

|Statistic | Value (pixels)| Value (mm)|
|------------------------|------|-----|
|Mean absolute difference|	0.457|	0.037|
|Mean difference|	0.03|	0.0024|
|Median difference|	0.0|	0.0|
|Min difference|	-9.0|	-0.72|
|Max difference|	20.0|	1.60|

Efter pixel-sÃ¸gningen er modellens absolutte gennemsnitlige afvigelse reduceret til 0.457 pixels, svarende til 0.037 mm fra den korrekte y-vÃ¦rdi. Den gennemsnitlige forskel ligger nu pÃ¥ 0.03, hvilket indikerer, at modellen kun i meget begrÃ¦nset omfang overskyder y-vÃ¦rdien.
BÃ¥de medianen og middelvÃ¦rdien for forskellene er faldet markant sammenlignet med fÃ¸r pixel-sÃ¸gningen, hvilket illustrerer en tydelig forbedring i modellens prÃ¦cision. Selvom der stadig findes enkelte outliers, er bÃ¥de minimums- og maksimumafvigelserne blevet mindre, hvilket viser, at de mest ekstreme fejl er reduceret.

Outputtet fra pixel matrixen kan findes i mappen **Pixel Matrix/Image Output**. Her er det muligt at downloade enhver html fil, Ã¥bne den i sin browser og interagere med visualiseringen. 

<table>
  <tr>
    <td><img src="Data/Figurer/Forudsagt_og_true_keypoint_1.png" width="275" height="180"/></td>
    <td><img src="Data/Figurer/Forudsagt_og_true_keypoint_2.png" width="275" height="180"/></td>
    <td><img src="Data/Figurer/Forudsagt_og_true_keypoint_3.png" width="275" height="180"/></td>
  </tr>
</table>

>  RÃ¸d: model punkt, grÃ¸n: ground truth punkt, blÃ¥: refined punkt og gul: sÃ¸gefelt (I overstÃ¥ende figur er der overlap mellem grÃ¸n og blÃ¥, og derfor er kun den ene synlig):

### Test med simulerede CSV-filer
Til sidst er modellen blevet testet pÃ¥ simulerede CSV-filer. Hver fil reprÃ¦senterer 75 patienter, hvor en vertikal translate key er angivet for hver patient. Denne nÃ¸gle bruges til at justere overkÃ¦ben vertikalt, sÃ¥ bÃ¥de over- og underkÃ¦be bringes ind i samme koordinatsystem og danner et â€œbidâ€.
Disse translate keys er udarbejdet af vores vejledere og sikrer, at hvert tandsÃ¦t i testdata har en bestemt grad af overbid. Vi har modtaget 10 af disse simulerede CSV-filer, som gÃ¸r det muligt at evaluere modellen i en simuleret virkelighed. Hvis en patients overbid ligger pÃ¥ grÃ¦nsen mellem to klasser, sÃ¥ klassificeres patienten med klassen None og bliver ekskluderet fra testen.
Ved brug af de evalueringsmetoder, der er beskrevet i det foregÃ¥ende afsnit, har vi opnÃ¥et fÃ¸lgende resultater for de 10 tests:


#### Detection Metrics (gÃ¦lder for alle tests)


#### FÃ¸r pixel-matrix-sÃ¸gning
| SDR (â‰¤ 0.5 mm) | SDR (â‰¤ 1 mm) | SDR (â‰¤ 2 mm) | Mean Radial Error (MRE) |
|----------------|--------------|--------------|--------------------------|
| 91 %           | 95.00 %      | 98.33 %      | 0.22 mm                  |


#### Efter pixel-matrix-sÃ¸gning
| SDR (â‰¤ 0.5 mm) | SDR (â‰¤ 1 mm) | SDR (â‰¤ 2 mm) | Mean Radial Error (MRE) |
|----------------|--------------|--------------|--------------------------|
| 89.33 %        | 96.00 %      | 99.00 %      | 0.12 mm                  |

#### Test Results

| Summary Name                | Classification Accuracy | Weighted Cohen's Kappa | Patients Total | Patients Excluded |
|----------------------------|--------------------------|------------------------|----------------|-------------------|
| Overbite_Classification1.csv | 97.22 %                 | 0.9927                 | 75             | 3                 |
| Overbite_Classification2.csv | 94.44 %                 | 0.9841                 | 75             | 3                 |
| Overbite_Classification3.csv | 95.89 %                 | 0.9906                 | 75             | 2                 |
| Overbite_Classification4.csv | 95.83 %                 | 0.9906                 | 75             | 3                 |
| Overbite_Classification5.csv | 94.59 %                 | 0.9860                 | 75             | 1                 |
| Overbite_Classification6.csv | 94.59 %                 | 0.9875                 | 75             | 1                 |
| Overbite_Classification7.csv | 95.89 %                 | 0.9894                 | 75             | 2                 |
| Overbite_Classification8.csv | 94.52 %                 | 0.9882                 | 75             | 2                 |
| Overbite_Classification9.csv | 97.30 %                 | 0.9935                 | 75             | 1                 |
| Overbite_Classification10.csv| 93.15 %                 | 0.9836                 | 75             | 2                 |

## Pipeline
Som den sidste del af projektet er der udviklet en pipeline, der tager to PLY-filer â€” Ã©n for overkÃ¦ben og Ã©n for underkÃ¦ben â€” som input. Pipelinen giver som output en visuel forudsigelse af det samlede tandsÃ¦t samt en klassifikation af overbid.

Den fÃ¸rste version af pipelinen byggede pÃ¥ kode, som vores vejleder havde stillet til rÃ¥dighed, oprindeligt udviklet af en postdoc. Denne kode accepterede Ã©n enkelt PLY-fil og genererede to billeder: Ã©t, der viste tÃ¦nderne fra venstre side, og Ã©t fra hÃ¸jre side.

Vi udvidede og Ã¦ndrede koden, sÃ¥ den nu tager to PLY-filer (over- og underkÃ¦be) som input. Vores tilfÃ¸jelser omfatter:

â€¢ Justering og ensretning af billederne

â€¢ KÃ¸rsel af modellen til overbidsklassifikation

â€¢ Fremstilling af et endeligt output, der viser bÃ¥de de forudsagte keypoints og klassifikationsresultatet

PÃ¥ nuvÃ¦rende tidspunkt kan vi ikke evaluere pipelinens ydeevne pÃ¥ virkelige eksempler. For at pipelinen kan give nÃ¸jagtige forudsigelser, skal bÃ¥de over- og underkÃ¦bemodellerne vÃ¦re i samme koordinatsystem. Denne justering findes dog ikke i de PLY-filer, vi har til rÃ¥dighed.


## Referencer 

Data:
Ben-Hamadou, Achraf, Neifar, Nour, Rekik, Ahmed, Smaoui, Oussama, Bouzguenda, Firas, Pujades, Sergi, Boyer, Edmond, and Ladroit, Edouard. "Teeth3Ds+: An Extended Benchmark for Intra-oral 3D Scans Analysis." arXiv preprint arXiv:2210.06094 (2022). https://arxiv.org/abs/2210.06094

