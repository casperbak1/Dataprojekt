# Overbidsklassificering â€” Automatisering og standardisering  
*Et maskinlÃ¦ringsprojekt til klassificering af overbid ud fra 3D-scanninger og billeder.*

---

## Table of Contents

- [Hurtigt Overblik](#hurtigt-overblik)
- [Projektbeskrivelse](#projektbeskrivelse)
  - [Abstract](#abstract)
  - [Introduktion](#introduktion)
  - [Data og Databehandling](#data-og-databehandling)
    - [Databehandling](#databehandling)
    - [Data](#data)
  - [CNN-netvÃ¦rk](#cnn-netvÃ¦rk)
  - [Pixel-matrix](#pixel-matrix)
  - [Evalueringsmetoder](#evalueringsmetoder)
  - [Resultater](#resultater)
  - [Pipeline](#pipeline)

---

## Hurtigt Overblik over filstruktur

- **Data:** Indeholder alle rÃ¥ og forarbejdede CSV- og PNG-filer, primÃ¦rt brugt til trÃ¦ning og test.
- **Overbite:** Indeholder kode, scripts, outputs og forskellige versioner brugt til overbite-klassificering.
- **Pipeline:** Viser hele workflowet fra 3D .PLY-fil til keypoint-placering.

---

## Mappe struktur
<details open>
  <summary>ğŸ“ Dataprojekt/</summary>
  <ul>
    <li>ğŸ“„ .gitignore</li>
    <li>ğŸ“„ Model.txt</li>
    <li>ğŸ“„ README.md</li>
    <li>
      <details>
        <summary>ğŸ“ Data/</summary>
        <ul>
          <li>ğŸ“„ pixel_flip_formula.png</li>
          <li>ğŸ“„ Splitting_and_flipping_of_images.ipynb</li>
          <li>
            <details>
              <summary>ğŸ“ Clean Data/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Bolton Data/</summary>
                    <ul>
                      <li>ğŸ“„ Example_lower_middle.png</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite Data/</summary>
                    <ul>
                      <li>ğŸ“„ Updated_Labels.csv</li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Data Pairs/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Test data/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Annotated Verication data/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Unannotated Data Pairs/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.png</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Figurer/</summary>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Raw Data/</summary>
              <ul>
                <li>ğŸ“„ 2024-04-08 Test data for overbite classification.xlsx</li>
                <li>ğŸ“„ 2025-05-08 TRANSLATE_KEY(1).xlsx</li>
                <li>ğŸ“„ Definitions of columns.docx</li>
                <li>ğŸ“„ Labels as of 19-02-2025 (Sample images).csv</li>
                <li>ğŸ“„ Labels as of 28-02-2025 (FINAL - for now).csv</li>
                <li>
                  <details>
                    <summary>ğŸ“ Sample images/</summary>
                    <ul>
                      <li>ğŸ“„ Example_lower_combined.png</li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
    <li>
      <details>
        <summary>ğŸ“ Overbite/</summary>
        <ul>
          <li>
            <details>
              <summary>ğŸ“ Kode/</summary>
              <ul>
                <li>ğŸ“„ Overbite.ipynb</li>
                <li>ğŸ“„ Pixel_Matrix_Optimizer.ipynb</li>
                <li>ğŸ“„ Test_Model.ipynb</li>
                <li>ğŸ“„ Train_Model.ipynb</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Other Versions (Overbite)/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Kode/</summary>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Output/</summary>
                    <ul>
                      <li>
                        <details>
                          <summary>ğŸ“ Modeller/</summary>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Overbite Detection/</summary>
                        </details>
                      </li>
                      <li>
                        <details>
                          <summary>ğŸ“ Pixel Matrix/</summary>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Output/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Keypoint Placement/</summary>
                    <ul>
                      <li>ğŸ“„ KP_Placement.csv</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Modeller/</summary>
                    <ul>
                      <li>ğŸ“„ Model.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite Detection/</summary>
                    <ul>
                      <li>ğŸ“„ All_FALSE_Classification_Rows.csv</li>
                      <li>ğŸ“„ Overbite_Classification9.csv</li>
                      <li>ğŸ“„ Results.ipynb</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Pixel Matrix/</summary>
                    <ul>
                      <li>ğŸ“„ KP_Refinement.csv</li>
                      <li>ğŸ“„ KP_Refinement_Distance.csv</li>
                      <li>
                        <details>
                          <summary>ğŸ“ Image Output/</summary>
                          <ul>
                            <li>ğŸ“„ Example_lower_left.html</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
    <li>
      <details>
        <summary>ğŸ“ Pipeline/</summary>
        <ul>
          <li>ğŸ“„ pipeline.ipynb</li>
          <li>ğŸ“„ README.txt</li>
          <li>
            <details>
              <summary>ğŸ“ docker_detectron2_env/</summary>
              <ul>
                <li>ğŸ“„ Dockerfile_pytorch3d_jupyter</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ output/</summary>
              <ul>
                <li>
                  <details>
                    <summary>ğŸ“ Overbite_Model/</summary>
                    </details>
                </li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Pipeline_code/</summary>
              <ul>
                <li>ğŸ“„ Opdeling_og_flip_af_billeder.py</li>
                <li>ğŸ“„ Overbite.py</li>
                <li>ğŸ“„ Pixelmatrix.py</li>
                <li>ğŸ“„ Ply_To_Image.py</li>
                <li>ğŸ“„ Run_model.py</li>
              </ul>
            </details>
          </li>
          <li>
            <details>
              <summary>ğŸ“ Pipeline_data/</summary>
              <ul>
                <li>ğŸ“„ patient_level_summary4.csv</li>
                <li>ğŸ“„ Predicted_keypoints.csv</li>
                <li>
                  <details>
                    <summary>ğŸ“ Clean Data/</summary>
                    <ul>
                      <li>
                        <details>
                          <summary>ğŸ“ Overbite Data/</summary>
                          <ul>
                            <li>ğŸ“„ Info.txt</li>
                          </ul>
                        </details>
                      </li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Model/</summary>
                    <ul>
                      <li>ğŸ“„ Info.txt</li>
                      <li>ğŸ“„ Model.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Output_after_pixel_matrix/</summary>
                    <ul>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Ply Files/</summary>
                    <ul>
                      <li>ğŸ“„ Brunatest LowerJawScan.ply</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan.ply</li>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
                <li>
                  <details>
                    <summary>ğŸ“ Raw_data/</summary>
                    <ul>
                      <li>ğŸ“„ Brunatest LowerJawScan_0.png</li>
                      <li>ğŸ“„ Brunatest LowerJawScan_1.png</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan_0.png</li>
                      <li>ğŸ“„ Brunatest_UpperJawScan_1.png</li>
                      <li>ğŸ“„ Info.txt</li>
                    </ul>
                  </details>
                </li>
              </ul>
            </details>
          </li>
        </ul>
      </details>
    </li>
  </ul>
</details>

---

# Projektbeskrivelse

## Indholdsfortegnelse

- Abstract
- Introduktion
- Data og databehandling
- CNN-netvÃ¦rk
- Resultater
- Pipeline

---

## Abstract

*Automatisering og standardisering af overbidsklassificering gennem maskinlÃ¦ring og billedbehandling.*

---

## Introduktion

I tandlÃ¦gepraksis findes der ikke Ã©n standardiseret metode til mÃ¥ling af overbid. Nogle anvender Ã¸jemÃ¥l, andre lineal, rÃ¸ntgenbilleder eller 3D-scanninger. Alle metoder har fordele og ulemper, men ofte er der en afvejning mellem prÃ¦cision og tidsforbrug.

**Overbid**

Overbid refererer til den vertikale afstand mellem de Ã¸verste og nederste fortÃ¦nder, mÃ¥lt fra spidsen af overkÃ¦bens fortÃ¦nder til spidsen af underkÃ¦bens. Et for stort overbid indikerer, at de Ã¸verste fortÃ¦nder dÃ¦kker en unormalt stor del af de nederste, hvilket kan have funktionelle og Ã¦stetiske konsekvenser.
Ved at mÃ¥le denne afstand kan man klassificere graden af overbid hos en patient og pÃ¥ den baggrund vurdere, om der er behov for behandling.
Et normalt overbid defineres typisk som en vertikal afstand pÃ¥ mellem 2 og 4 mm.
I dette projekt er fÃ¸lgende klassifikationer blevet anvendt som udgangspunkt for evaluering og test af modellen. Disse klasser er udarbejdet af vores vejledere:

| Klasse | Vertikal afstand |
|--------|------------------|
| A      | < 1 mm           |
| B      | 1â€“2 mm           |
| C      | 2â€“3 mm           |
| D      | 3â€“4 mm           |
| E      | > 4 mm           |


**Dette projekt har to hovedfokusomrÃ¥der:**

1. Udvikling og trÃ¦ning af maskinlÃ¦ringsmodeller til prÃ¦cis overbidsklassificering.
2. Udarbejdelse af en pipeline, der gÃ¥r fra 3D-filer (.PLY) til keypoint-markering, der muliggÃ¸r automatisk mÃ¥ling.

**BemÃ¦rk:** Resten af denne projektbeskrivelse omhandler kun punkt 1. Pipeline beskrives separat i sektionen "Pipeline".

Projektet bygger pÃ¥ et offentligt datasÃ¦t bestÃ¥ende af 1.351 tredimensionelle intraorale scanninger. Til dette projekt er 3D-scanningerne konverteret til todimensionelle billeder, hvilket gÃ¸r det muligt at anvende gÃ¦ngse deep-learning-metoder til billedanalyse. 

Som model anvender vi Keypoint R-CNN, en videreudvikling af Mask R-CNN, der er designet til at finde prÃ¦cise keypoints i billeder. Ved at kombinere regionsÂ­forslag med punktdetektion gÃ¸r modellen det muligt at identificere nÃ¸jagtige punkter pÃ¥ tÃ¦nder, hvilket er afgÃ¸rende for vores opgave, da overbid mÃ¥les som den vertikale afstand mellem to prÃ¦cise punkter pÃ¥ fortÃ¦nderne.

---

## Data og Databehandling

### Databehandling

Projektet startede med **1351 kombinerede billeder** (3 vinkler pr. patient):

<img src="Data/Figurer/00OMSZGW_lower_combined.png" width="800" height="250"/>

> *Billede af underkÃ¦be fra 3 vinkler*

Til billederne var der **1166 annoteringer**, eksempelvis:

| Filename                       | X1  | Y1  | X2   | Y2  |
|---------------------------------|-----|-----|------|-----|
| 00OMSZGW_lower_combined.png     | 777 | 492 | 2310 | 487 |

#### MÃ¥l for databehandling:

1. Split kombinerede billeder til tre separate billeder (left, middle, right)
2. Fordel billeder i mapper: **Bolton Data** og **Overbite Data**
3. Ensret orientering og koordinationer i "Overbite Data" (ved transponering)
4. GruppÃ©r data som billed-par
5. Del data i trÃ¦ning, test, verifikation, og uannoteret data

#### Step 1: Opdeling af billeder

<table>
  <tr>
    <td><img src="Data/Figurer/Lower_Right.png" width="250" height="250"/></td>
    <td><img src="Data/Figurer/Lower_Middle.png" width="250" height="250"/></td>
    <td><img src="Data/Figurer/Lower_Left.png" width="250" height="250"/></td>
  </tr>
</table>


#### Step 2: Sortering

- **Left/Right** billeder â†’ *Overbite Data*
- **Middle** billede â†’ *Bolton Data*

#### Step 3: Ensretning

"Right" billedet transponeres (flippes), sÃ¥ orienteringen matcher "Left" billedet, og koordinater justeres:\

<table>
  <tr>
    <td><img src="Data/Figurer/Lower_Right.png" width="400" height="400"/></td>
    <td><img src="Data/Figurer/Lower_Left_Flipped.png" width="400" height="400"/></td>
  </tr>
</table>

> Keypoint/Annoteringen er markeret med rÃ¸d pÃ¥ de 2 billeder

Koordinatet justeres sÃ¥ med fÃ¸lgende formel:
`x' = w - 1 - x`\
Her er `x = X2 = 2310` fra originalen, w = 3072 fra originalen, og derfor er det nye x beregnet til:\
`x' = 3072 - 1 - 2310 = 761`

Og den nye CSV vil derfor blive til:

| Filename                   | X1  | Y1  |
|----------------------------|-----|-----|
| 00OMSZGW_lower_left.png   | 777 | 492 |
| 00OMSZGW_lower_right.png    | 761 | 487 |


#### Step 4 + 5: Par og grupper

- Et *par* = 4 billeder med koordinater (left/right, upper/lower)
- UfuldstÃ¦ndige par â†’ *Unannotated Data Pairs*
- Resten inddeles i:
    - *Annotated Data Pairs* (trÃ¦ning)
    - *Annotated Verication data* (verifikation under trÃ¦ning)
    - *Annotated Test data* (test efter trÃ¦ning)

> Outliers og fejl rettes i CSV efter databehandling.  
> Alt databehandling findes i `Splitting_and_flipping_of_images.ipynb`.


---

Oversigt over fordelingen af billeder:

| Folder                               | Image count | Patient count | Annotated |
|---------------------------------------|-------------|-------------|-------------|
| Bolton Data                          | 1351        | 675 | No |
| Overbite Data                        | 2702        | 675 | NaN |
| Overbite Data/Annotated Data Pairs   | 1580        | 395 | Yes |
| Overbite Data/Annotated Verication data | 100      | 25 | Yes |
| Overbite Data/Annotated Test data    | 300         | 75 | Yes |
| Overbite Data/Unannotated Data Pairs | 722         | 180 | NaN|

Efter dataen er opdelt i mapper mangler vi kun 2 ting inden trÃ¦ningen kan pÃ¥begynde:

1. OpsÃ¦tte en bounding boks
2. Konvertere annoteringer til "COCO JSON" format

Vi opsÃ¦tter en bounding boks for at hjÃ¦lpe med at lokalisere objektet af interesse. Det hjÃ¦lper modellen med at finde det omrÃ¥de den skal fokusere pÃ¥, for at lave forudsigelser. Dette gÃ¸r modellen mere prÃ¦cis og effektiv.

Annoteringerne skal til sidst konverteres til et format som kan lÃ¦ses af modellen.

Modellen modtager altsÃ¥ som input til trÃ¦ning 1580 billeder af fÃ¸lgende format:

<img src="Data/Figurer/Data_For_Training.png" width="600" height="600"/>

> *Keypoint markeret med rÃ¸d, og bounding box markeret med grÃ¥*

Og annoteringen:

```json
{
  "images": [
    {
      "file_name": "Dataprojekt/Data/Clean Data/Overbite Data/Annotated Data Pairs/00OMSZGW_lower_left.png",
      "height": 1024,
      "width": 1024,
      "id": 0
    }
  ],
  "annotations": [
    {
      "bbox": [740, 492, 62, 125],
      "bbox_mode": 1,
      "category_id": 0,
      "keypoints": [777, 492, 2],
      "num_keypoints": 1,
      "image_id": 0,
      "id": 0
    }
  ],
  "categories": [
    {
      "id": 0,
      "name": "tooth",
      "keypoints": ["apex"],
      "skeleton": []
    }
  ]
}
```

Modellen er altsÃ¥ nu klar til at blive trÃ¦net.


---

## Keypoints R-CNN-netvÃ¦rk

Vi trÃ¦nede vores model ved at benytte os af en forudtrÃ¦net model fra Detectron2.\
Den model vi har anvendt hedder "keypoint_rcnn_X_101_32x8d_FPN_3x".\
Modellen er altsÃ¥ en Keypoint R-CNN model hvor X_101_32x8d_FPN er selve backbone-arkitekturen:\
X_101_32x8d betyder ResNeXt-101 (101 lag) med 32 grupper og en bredde pÃ¥ 8 per gruppe\
FPN stÃ¥r for Feature Pyramid Network, hvilket betyder, at modellen bruger flere â€œfeature-mapsâ€ pÃ¥ forskellige skalaer\

Den grundlÃ¦ggende mÃ¥de vores trÃ¦ning har kÃ¸rt pÃ¥ er:

Vi starter med billeder af stÃ¸rrelsen 1024x1024 som hver har et ground truth keypoint.
Det nÃ¦ste der sker er at billederne bliver augmenteret pÃ¥ forskellige mÃ¥der, f.eks. med andre billedstÃ¸rrelser, spejlinger, lys- og kontrastjusteringer sÃ¥ modellen ikke kun lÃ¦rer et specifikt udseende. Annoteringerne fÃ¸lger selvfÃ¸lgelig med spejlinger osv.
Denne augmentering er med til at gÃ¸re modellen mere robust.

Det nÃ¦ste der sker er at vi indlÃ¦ser billederne i batches.
Dette gÃ¸r vi fordi at havde vi kun taget et billede af gangen, havde vi fÃ¥et langsommere trÃ¦ning og en mere ustabil trÃ¦ning.
NÃ¥r vi trÃ¦ner modellen med en batch str. pÃ¥ 16, sÃ¥ vil modellens vÃ¦gte blive opdateret pÃ¥ gennemsnittet af fejlen for hele batchen. Havde vi nu kun brugt 1 billede af gangen, sÃ¥ ville modellens vÃ¦gte blive opdateret pÃ¥ baggrund af fejlen for dette ene billede og havde billedet nu vÃ¦ret en outlier, sÃ¥ ville modellens vÃ¦gte altsÃ¥ blive opdateret i retning af en outlier.

Efter augmentering og batch indlÃ¦sning kommer vi til ResNeXt-101, dette er selve CNN delen.
Her har vi 101 lag, som hver laver en convulution.
Hver enkelt convulution bliver sÃ¥ indelt i 32 grupper.
De 32 grupper finder sÃ¥ mÃ¸nstre.
Efter ResNeXt-101 ender vi op med en masse "Feature maps" hvor hver pixel svarer til styrken af et bestemt mÃ¸nster i billedet.

Nu har vi en masse feature maps og det er her FPN kommer ind i billedet.
FPN samler feature maps fra alle de forskellige lag.
De tidligste lag finder de helt smÃ¥ mÃ¸nstre, og de sene lag finder helhedderne.

Vi er nu ankommet til RPN (Region Proposal Network).
RPN foreslÃ¥r tusindvis af smÃ¥ bokse med forskellige stÃ¸rrelser og former alle  de steder, hvor der kan vÃ¦re noget spÃ¦ndende.
For hver enkelt af disse bokse scorer RPN, hvor sandsynligt det er, at boksen indeholder et objekt. De bedste forslag gÃ¥r videre til nÃ¦ste skridt.

Vi er nu ved at vÃ¦re ved vejs ende.
For hver af de vidersendte forslag fra RPN skaber modellen et heatmap hvor hver pixel viser sandsynligheden for at vores keypoint ligger der.
Sandsynlighederne laves med en softmax-funktion, sÃ¥ de summer til 1.
SÃ¥ tager modellen den pixel pÃ¥ heatmappet, hvor sandsynligheden er hÃ¸jest, da det er modellens bedste bud pÃ¥ keypointets placering.

Til sidst er der loss, backpropagation og optimering.
Efter modellen har givet sit bedste bud pÃ¥ hvor keypointet skal placeres, sÃ¥ beregnes det hvor galt den tog fejl, med en cross-entropy loss mellem predicted og ground truth.
Efter det sker der backpropagation;
Den starter med fejlen og regner, hvor meget hver vÃ¦gt i netvÃ¦rket har pÃ¥virket fejlen, dette sker lag-for-lag bagud gennem hele netvÃ¦rket med kÃ¦dereglen for differenciering.
VÃ¦gtene justeres en smule, sÃ¥ fejlen bliver mindre nÃ¦ste gang.
Det her gentager vi for hver batch indtil modellen er konvergeret.

---

## Pixel-matrix

Efter modellen har placeret keypoints pÃ¥ billederne, finjusteres disse positioner ved hjÃ¦lp af pixelsÃ¸gning. Dette sker ved at definere en matrix (af forudbestemt stÃ¸rrelse) omkring det keypoint, som modellen har forudsagt. Inden for denne matrix identificeres den pixel, der ligger hÃ¸jest og er lysest, og denne pixel vÃ¦lges herefter som det nye, justerede keypoint. Hvis der findes to pixels med samme vÃ¦rdi, bliver den pixel, der ligger lÃ¦ngst til venstre, valgt. Dette valg bygger pÃ¥ hvordan det sande keypoint bliver annoteret. 
FormÃ¥let med denne proces er at sikre, at keypointet placeres sÃ¥ prÃ¦cist som muligt ud fra det forudsagte keypoint fra modellen, hvilket forbedrer den samlede nÃ¸jagtighed og robusthed af keypoint placeringen.

Figuren viser fordelingen af fejl for placeringen af keypoints efter pixelsÃ¸gningen er implementeret.

<img src="Data/Figurer/Histogram_efter_pixelmatrix.png" width="900" height="400"/> 

## Evalueringsmetoder

Til evaluering af modellen har vi benyttet os af tre metoder:

1. **Mean radial error (MRE)**
2. **Succesful detection rate (SDR)**
3. **Weighted Cohenâ€™s kappa**

De er defineret som fÃ¸lger:

**Mean radial error**:

$$
MRE = \frac{\sum_{i=1}^N R_i}{N}
$$

Her er \$N\$ antallet af predikterede punkter, og \$R\_i\$ er den euklidiske afstand mellem ground truth og modellens punkt.

MRE giver os indsigt i, hvor tÃ¦t modellens forudsigelser i gennemsnit ligger pÃ¥ det korrekte punkt (ground truth). Jo lavere MRE, desto mere prÃ¦cis er modellens gennemsnitlige punktplacering.

**Succesful detection rate**:

$$
SDR = \frac{K}{N} \cdot 100
$$

Her er \$N\$ antallet af predikterede punkter, og \$K\$ er antallet af korrekt placerede punkter indenfor et tilladt "fejl"-interval. Vi har brugt intervaller, der tillader 0.5, 1 og 2 mm afstand i forhold til ground truth.

SDR viser, hvor stor en andel af modellens forudsigelser der ligger indenfor et givent toleranceniveau fra ground truth (fx 0.5, 1 eller 2 mm). Det er sÃ¦rligt nyttigt, hvis man vil vide, hvor ofte modellen rammer â€œtilstrÃ¦kkeligt tÃ¦tâ€ pÃ¥ det korrekte punkt, givet at man accepterer en vis fejlmargin.

**Weighted Cohenâ€™s kappa** (\$\kappa\_w\$) bruges til at mÃ¥le, hvor god overensstemmelse der er mellem modellens klassifikation og den sande (ekspert-annoterede) klasse, hvor tilfÃ¦ldig overensstemmelse er korrigeret for:

$$
\kappa_w = 1 - \frac{\sum_{i,j} w_{i,j} O_{i,j}}{\sum_{i,j} w_{i,j} E_{i,j}}
$$

Her er \$O\_{i,j}\$ andelen af tilfÃ¦lde, hvor ground truth er klasse \$i\$ og modellen valgte klasse \$j\$, og \$E\_{i,j}\$ er den forventede andel af sÃ¥danne tilfÃ¦lde, hvis annotatorerne var uafhÃ¦ngige.

VÃ¦gtene \$w\_{i,j}\$ bruges til at straffe stÃ¸rre fejl hÃ¥rdere. Ved **kvadratisk vÃ¦gtning** (â€œquadratic weightsâ€) beregnes vÃ¦gten som:

$$
w_{i,j} = \left( \frac{i - j}{k - 1} \right)^2
$$

hvor \$k\$ er antallet af klasser (her 5 for â€œAâ€â€“â€œEâ€).

Det vil sige, at hvis modellen forveksler â€œAâ€ og â€œEâ€, tÃ¦ller det som en stÃ¸rre fejl end hvis den forveksler â€œAâ€ og â€œBâ€.

---

## Resultater
Projektets resultater bestÃ¥r primÃ¦rt af:

* CSV-filer med forudsigelser og mÃ¥linger
* Visualiseringer i form af histogrammer
* Billeder med keypoints placeret af modellen

### Modeller og gemte filer

Der er blevet trÃ¦net en rÃ¦kke modeller, som er placeret i mappen `Overbite/Other Versions (Overbite)/`.
Ã‰n af modellerne er udvalgt som den endelige, men flere af de Ã¸vrige modeller prÃ¦sterer nÃ¦sten lige sÃ¥ godt.

Ved afslutningen af trÃ¦ningen gemmes den valgte model som en `.pth`-fil, som anvendes sammen med biblioteket Detectron2.
Da filen overstiger GitHubs uploadgrÃ¦nse, er den ikke inkluderet direkte i repositoryet. Der findes dog et link til filen i den relevante mappe.

### Keypoint-placering

I mappen `Overbite/Output/Keypoint Placement/` ligger filen `KP_Placement.csv`.
Denne indeholder bÃ¥de de sande (ground truth) og de forudsagte keypoints samt den euklidiske distance mellem disse mÃ¥lt i pixels og millimeter.

Eksempel pÃ¥ indhold:

| Filename              | X\_True | Y\_True | X\_Model | Y\_Model | Euc\_dist | mm\_dist |
| --------------------- | ------- | ------- | -------- | -------- | --------- | -------- |
| 013FHA7K\_lower\_left | 844     | 369     | 842.22   | 368.63   | 1.82      | 0.15     |

### Fordeling over afstanden

Nedenfor vises histogrammer over de euklidiske afstande mellem modelens forudsigelser og de sande punkter (ground truth).

<img src="Data/Figurer/Histogram_0_6mm.png" width="600" height="412"/>

> Uden outliers > 0.6 mm. 276/300 Resultater.

<img src="Data/Figurer/Histogram_1_mm.png" width="562" height="380"/>

> Uden outliers > 1 mm. 285/300 Resultater.

<img src="Data/Figurer/Histogram_3_5mm.png" width="576" height="379"/>

> Uden outliers > 3.5 mm. 300/300 Resultater.

### Detection Metrics (gÃ¦lder for alle tests)

| SDR (â‰¤ 0.5 mm) | SDR (â‰¤ 1 mm) | SDR (â‰¤ 2 mm) | Mean Radial Error (MRE) |
|----------------|--------------|--------------|--------------------------|
| 89.33 %        | 96.00 %      | 99.00 %      | 0.22 mm                  |

### Test Results

| Summary Name                | Classification Accuracy | Weighted Cohen's Kappa | Patients Total | Patients Excluded |
|----------------------------|--------------------------|------------------------|----------------|-------------------|
| Overbite_Classification1.csv | 97.22 %                 | 0.9927                 | 75             | 3                 |
| Overbite_Classification2.csv | 94.44 %                 | 0.9841                 | 75             | 3                 |
| Overbite_Classification3.csv | 95.89 %                 | 0.9906                 | 75             | 2                 |
| Overbite_Classification4.csv | 95.83 %                 | 0.9906                 | 75             | 3                 |
| Overbite_Classification5.csv | 94.59 %                 | 0.9860                 | 75             | 1                 |
| Overbite_Classification6.csv | 94.59 %                 | 0.9875                 | 75             | 1                 |
| Overbite_Classification7.csv | 95.89 %                 | 0.9894                 | 75             | 2                 |
| Overbite_Classification8.csv | 94.52 %                 | 0.9882                 | 75             | 2                 |
| Overbite_Classification9.csv | 97.30 %                 | 0.9935                 | 75             | 1                 |
| Overbite_Classification10.csv| 93.15 %                 | 0.9836                 | 75             | 2                 |


## Pipeline
Som den sidste del af projektet har vi udviklet en pipeline, der tager to PLY-filer â€” Ã©n for overkÃ¦ben og Ã©n for underkÃ¦ben â€” som input. Pipelinen giver som output en visuel forudsigelse af det samlede tandsÃ¦t samt en klassifikation af overbid.

Den fÃ¸rste version af pipelinen byggede pÃ¥ kode, som vores vejleder havde stillet til rÃ¥dighed, oprindeligt udviklet af en postdoc. Denne kode accepterede Ã©n enkelt PLY-fil og genererede to billeder: Ã©t, der viste tÃ¦nderne fra venstre side, og Ã©t fra hÃ¸jre side.

Vi udvidede og Ã¦ndrede koden, sÃ¥ den nu tager to PLY-filer (over- og underkÃ¦be) som input. Vores tilfÃ¸jelser omfatter:

â€¢ Justering og ensretning af billederne

â€¢ KÃ¸rsel af modellen til overbidsklassifikation

â€¢ Fremstilling af et endeligt output, der viser bÃ¥de de forudsagte keypoints og klassifikationsresultatet

PÃ¥ nuvÃ¦rende tidspunkt kan vi ikke evaluere pipelinens ydeevne pÃ¥ virkelige eksempler. For at pipelinen kan give nÃ¸jagtige forudsigelser, skal bÃ¥de over- og underkÃ¦bemodellerne vÃ¦re i samme koordinatsystem. Denne justering findes dog ikke i de PLY-filer, vi har til rÃ¥dighed.


## Referencer 

Data:
Ben-Hamadou, Achraf, Neifar, Nour, Rekik, Ahmed, Smaoui, Oussama, Bouzguenda, Firas, Pujades, Sergi, Boyer, Edmond, and Ladroit, Edouard. "Teeth3Ds+: An Extended Benchmark for Intra-oral 3D Scans Analysis." arXiv preprint arXiv:2210.06094 (2022). https://arxiv.org/abs/2210.06094

